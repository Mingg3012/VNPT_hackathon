import json
import re
import time
import requests
import chromadb
from tqdm import tqdm
import config # File config c·ªßa b·∫°n
import sys
import pandas as pd

try:
    __import__('pysqlite3')
    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
except ImportError:
    pass

# --- C·∫§U H√åNH ---
BLACKLIST_KEYWORDS = [

    # 1. T√¨nh d·ª•c ‚Äì khi√™u d√¢m
    "t√¨nh d·ª•c", "khi√™u d√¢m", "khi√™u d√¢m tr·∫ª em", "d√¢m √¥",
    "hi·∫øp d√¢m", "c∆∞·ª°ng hi·∫øp", "m·∫°i d√¢m", "k√≠ch d·ª•c", 
    "·∫£nh n√≥ng", "clip n√≥ng", "th·ªß d√¢m", "lo·∫°n lu√¢n", 
    "mua d√¢m", "b√°n d√¢m", "·∫•u d√¢m", 

    # # 2. Ma t√∫y ‚Äì ch·∫•t c·∫•m
    # "heroin", "cocaine", "c·∫ßn sa", "thu·ªëc l·∫Øc", "meth",
    # "bu√¥n b√°n ma t√∫y", "ch√≠ch ma t√∫y", "pha ch·∫ø ma t√∫y",
    
    # # 3. C·ªù b·∫°c ‚Äì c√° ƒë·ªô (Gi·ªØ c√°c nh√† c√°i n·ªïi b·∫≠t)
    # "c·ªù b·∫°c", "ƒë√°nh b·∫°c", "c√° ƒë·ªô", "l√¥ ƒë·ªÅ", "x·ªï s·ªë l·∫≠u",
    # "ƒë√°nh b√†i ƒÉn ti·ªÅn", "nh√† c√°i", 
    # "1xbet", "fun88", "m88", "w88", "fb88", "8xbet", "bet365", "onbet", "letou", "melbet", "men88",

    # # 4. B·∫°o l·ª±c ‚Äì gi·∫øt ch√≥c ‚Äì kh·ªßng b·ªë
    # "kh·ªßng b·ªë", "ƒë√°nh bom", "√°m s√°t", "gi·∫øt ng∆∞·ªùi",
    # "th·∫£m s√°t", "ch·∫∑t ƒë·∫ßu", "x·∫£ s√∫ng",
    # "ch·∫ø t·∫°o bom", "ch·∫ø t·∫°o v≈© kh√≠",

    # # 5. V≈© kh√≠ & chi·∫øn tranh (phi h·ªçc thu·∫≠t)
    # # Gi·ªØ c√°c t·ª´ li√™n quan ƒë·∫øn ch·∫ø t·∫°o/bu√¥n b√°n
    # "ch·∫ø t·∫°o v≈© kh√≠", "mua b√°n v≈© kh√≠",
    # "bu√¥n l·∫≠u v≈© kh√≠",

    # # 6. T·ª± t·ª≠ ‚Äì t·ª± h·∫°i ‚Äì r·ªëi lo·∫°n t√¢m l√Ω nguy c·∫•p
    # "t·ª± t·ª≠", "t·ª± s√°t", "t·ª± h·∫°i", "mu·ªën ch·∫øt",
    # "k·∫øt li·ªÖu b·∫£n th√¢n", "u·ªëng thu·ªëc t·ª± t·ª≠",
    # "nh·∫£y l·∫ßu", "c·∫Øt c·ªï tay",

    # # 7. Hacking ‚Äì an ninh m·∫°ng ‚Äì x√¢m nh·∫≠p tr√°i ph√©p
    # "hacking", "hack", "b·∫ª kh√≥a", "crack",
    # "x√¢m nh·∫≠p tr√°i ph√©p", "ƒë√°nh c·∫Øp d·ªØ li·ªáu",
    # "t·∫•n c√¥ng m·∫°ng", "ddos", "phishing",
    # "keylogger", "malware", "virus m√°y t√≠nh",
    # "chi·∫øm quy·ªÅn ƒëi·ªÅu khi·ªÉn",

    # # 8. L·ª´a ƒë·∫£o ‚Äì t·ªôi ph·∫°m kinh t·∫ø ‚Äì t√†i ch√≠nh
    # "l·ª´a ƒë·∫£o", "chi·∫øm ƒëo·∫°t t√†i s·∫£n", "ƒëa c·∫•p",
    # "r·ª≠a ti·ªÅn", "tham nh≈©ng", "h·ªëi l·ªô",
    # "tr·ªën thu·∫ø", "l√†m gi·∫£ gi·∫•y t·ªù",
    # "l·ª´a ƒë·∫£o tr·ª±c tuy·∫øn", "gian l·∫≠n t√†i ch√≠nh",

    # # 9. Th√π h·∫≠n ‚Äì x√∫c ph·∫°m ‚Äì ph√¢n bi·ªát
    # "ph√¢n bi·ªát ch·ªßng t·ªôc", "k·ª≥ th·ªã", "th√π h·∫±n",
    # "x√∫c ph·∫°m", "lƒÉng m·∫°", "mi·ªát th·ªã",
    # "ch·ª≠i b·ªõi", "b√¥i nh·ªç", "vu kh·ªëng",
    # "k√≠ch ƒë·ªông th√π gh√©t",

    # # 10. Ch√≠nh tr·ªã c·ª±c ƒëoan / ch·ªëng ph√° (ngo√†i h·ªçc thu·∫≠t)
    # "l·∫≠t ƒë·ªï", "ch·ªëng ph√° nh√† n∆∞·ªõc",
    # "bi·ªÉu t√¨nh b·∫°o lo·∫°n", "b·∫°o lo·∫°n",
    # "ly khai", "tuy√™n truy·ªÅn ph·∫£n ƒë·ªông",
    # "ch·ªß nghƒ©a c·ª±c ƒëoan",

    # # 11. T·ªôi ph·∫°m con ng∆∞·ªùi & gia ƒë√¨nh
    # "mua b√°n ng∆∞·ªùi", "bu√¥n ng∆∞·ªùi",
    # "x√¢m h·∫°i tr·∫ª em", "b·∫°o h√†nh gia ƒë√¨nh",
    # "b·∫Øt c√≥c", "tra t·∫•n", "ng∆∞·ª£c ƒë√£i",

    # # 12. H√†nh vi tr√°i ph√°p lu·∫≠t kh√°c
    # "vi ph·∫°m ph√°p lu·∫≠t", "h√†nh vi ph·∫°m t·ªôi",
    # "che gi·∫•u t·ªôi ph·∫°m", "ti√™u th·ª• t√†i s·∫£n ph·∫°m ph√°p",
    # "ƒë∆∞·ªùng d√¢y t·ªôi ph·∫°m",

    # # 13. X√¢m ph·∫°m quy·ªÅn ri√™ng t∆∞ & Doxing
    # "doxing", "t√¨m info", "tra c·ª©u th√¥ng tin c√° nh√¢n",
    # "s·ªë cccd", "s·ªë ch·ª©ng minh th∆∞", "l·ªô clip ri√™ng t∆∞", 
    # "quay l√©n", "camera quay l√©n", "theo d√µi v·ªã tr√≠",
    # "ƒÉn c·∫Øp danh t√≠nh", "gi·∫£ m·∫°o danh t√≠nh",

    # # 14. Tin gi·∫£, Deepfake & Thao t√∫ng th√¥ng tin
    # "deepfake", "gh√©p m·∫∑t", "gi·∫£ gi·ªçng n√≥i", 
    # "tung tin ƒë·ªìn th·∫•t thi·ªát", "ch·ªânh s·ª≠a ·∫£nh nh·∫°y c·∫£m", "gh√©p ·∫£nh n√≥ng",

    # # 15. H√†ng c·∫•m & ƒê·ªông v·∫≠t hoang d√£
    # "ng√† voi", "s·ª´ng t√™ gi√°c", "m·∫≠t g·∫•u", "v·∫£y t√™ t√™",
    # "ƒë·ªông v·∫≠t s√°ch ƒë·ªè", "bu√¥n l·∫≠u ƒë·ªông v·∫≠t",
    # "ti·ªÅn gi·∫£", "in ti·ªÅn gi·∫£", 
    # "l√†m b·∫±ng gi·∫£", "l√†m gi·∫•y t·ªù gi·∫£", "b·∫±ng l√°i xe gi·∫£",

    # # 16. B·∫Øt n·∫°t qua m·∫°ng (Cyberbullying) & Qu·∫•y r·ªëi
    # "b√≥c ph·ªët", "t·∫©y chay", "d√¨m h√†ng", "n√©m ƒë√° h·ªôi ƒë·ªìng",
    # "body shaming", "mi·ªát th·ªã ngo·∫°i h√¨nh", "c√¥ng k√≠ch c√° nh√¢n",
    # "stalking", "b√°m ƒëu√¥i", "qu·∫•y r·ªëi tin nh·∫Øn", "ƒëe d·ªça tung ·∫£nh",

    # # 17. T·ªá n·∫°n x√£ h·ªôi & D·ªãch v·ª• phi ph√°p kh√°c
    # "ƒë√≤i n·ª£ thu√™", "si·∫øt n·ª£", "t√≠n d·ª•ng ƒëen", "vay n·∫∑ng l√£i",
    # "b·ªëc b√°t h·ªç", "cho vay l√£i c·∫Øt c·ªï",
    # "mang thai h·ªô (th∆∞∆°ng m·∫°i)", "ƒë·∫ª thu√™", "b√°n th·∫≠n", "b√°n n·ªôi t·∫°ng",
    # "k·∫øt h√¥n gi·∫£", "v∆∞·ª£t bi√™n tr√°i ph√©p",

    # # 18. T·ª´ l√≥ng/Vi·∫øt t·∫Øt th∆∞·ªùng d√πng ƒë·ªÉ l√°ch lu·∫≠t
    # "k·∫πo ke" , "bay l·∫Øc", "x√†o ke", "h√†ng tr·∫Øng", "ƒë√°", "ma t√∫y ƒë√°",
    # "g√† m√≥ng ƒë·ªè" , "m·∫°i d√¢m", "checker" , 
    # "sugar baby", "sugar daddy" , 
    # "child porn",  
    
    # # 19. Vi ph·∫°m b·∫£n quy·ªÅn & Ph·∫ßn m·ªÅm l·∫≠u (T·∫≠p trung v√†o h√†nh vi)
    # "crack win", "crack office", "b·∫ª kh√≥a ph·∫ßn m·ªÅm",
    # "xem phim l·∫≠u", "t·∫£i game crack",
    # "tool hack game",

    # # 20. Gian l·∫≠n thi c·ª≠ & H·ªçc thu·∫≠t (Gi·ªØ c√°c h√†nh vi tr·ª±c ti·∫øp)
    # "thi h·ªô", "h·ªçc h·ªô", "l√†m b√†i thu√™", "vi·∫øt lu·∫≠n vƒÉn thu√™",
    # "mua b·∫±ng ƒë·∫°i h·ªçc", "l√†m gi·∫£ b·∫±ng c·∫•p", "ch·∫°y ƒëi·ªÉm",
    # "phao thi", "tai nghe si√™u nh·ªè", "camera c√∫c √°o",
    # "mua ƒë·ªÅ thi", "l·ªô ƒë·ªÅ thi",
    # "ghostwriter", "d·ªãch v·ª• vi·∫øt thu√™",

    # # 21. Y t·∫ø sai l·ªách & S·ª©c kh·ªèe ƒë·ªôc h·∫°i
    # "thu·ªëc k√≠ch d·ª•c n·ªØ", "thu·ªëc m√™", "b√°n thu·ªëc ph√° thai",
    # "pro-ana", "m√≥c h·ªçng gi·∫£m c√¢n",

    # # 22. L·ª´a ƒë·∫£o tuy·ªÉn d·ª•ng & Vi·ªác l√†m
    # "vi·ªác nh·∫π l∆∞∆°ng cao", "ng·ªìi nh√† ki·∫øm ti·ªÅn", 
    # "n·∫°p ti·ªÅn nh·∫≠n th∆∞·ªüng", "ƒë·∫ßu t∆∞ sinh l·ªùi 100%",
    # "xu·∫•t kh·∫©u lao ƒë·ªông chui",

    # # 23. Ph√¢n bi·ªát v√πng mi·ªÅn (Gi·ªØ c√°c t·ª´ nh·∫°y c·∫£m tr·ª±c ti·∫øp)
    # "parky", "nam c·∫ßy", "t·ªôc c·ªëi",
    # "ph√¢n bi·ªát v√πng mi·ªÅn", "pbvm",

    # # 24. T√¥n gi√°o m√™ t√≠n & T√† gi√°o
    # "b√πa ng·∫£i", "y·ªÉm b√πa", "nu√¥i kumanthong", "ch∆°i ng·∫£i",
    # "h·ªôi th√°nh ƒë·ª©c ch√∫a tr·ªùi", "t√† ƒë·∫°o", "truy·ªÅn ƒë·∫°o tr√°i ph√©p",
    # "l√™n ƒë·ªìng l·ª´a ƒë·∫£o", "tr·ª•c vong thu ti·ªÅn",

    # # 25. T·ª´ kh√≥a l√≥ng/Code m·ªõi c·ªßa gi·ªõi tr·∫ª (Ch·ªâ gi·ªØ t·ª´ l√≥ng th√¥ t·ª•c)
    # "x·∫øp h√¨nh" , "ch·ªãch", "xo·∫°c",
    # "n·ª©ng", "h·ª©ng", "buscu", "v√©t m√°ng",

    # # 26. C√°c lo·∫°i bom/v≈© kh√≠ t·ª± ch·∫ø (T·∫≠p trung v√†o ch·∫ø t·∫°o/nguy hi·ªÉm)
    # "bom xƒÉng", "bom kh√≥i", "ch·∫ø ph√°o", "thu·ªëc ph√°o",
    # "dao ph√≥ng l·ª£n", "m√£ t·∫•u", "ki·∫øm nh·∫≠t",
    # "s√∫ng c·ªìn", "s√∫ng b·∫Øn bi", "n√° thun s√°t th∆∞∆°ng",

    # # 27. Khai th√°c tr·∫ª v·ªã th√†nh ni√™n & grooming (Gi·ªØ nguy√™n)
    # "grooming", "d·ª• d·ªó tr·∫ª em", "chat sex v·ªõi tr·∫ª em",
    # "quan h·ªá v·ªõi tr·∫ª v·ªã th√†nh ni√™n",

    # # 28. T·∫•n c√¥ng sinh h·ªçc ‚Äì h√≥a h·ªçc (Gi·ªØ nguy√™n)
    # "nu√¥i vi khu·∫©n", "t·∫°o virus", "ph√°t t√°n d·ªãch b·ªánh",
    # "ch·∫ø t·∫°o ch·∫•t ƒë·ªôc", "ph√°t t√°n kh√≠ ƒë·ªôc",
    # "v≈© kh√≠ sinh h·ªçc t·ª± ch·∫ø",

    # # 29. H∆∞·ªõng d·∫´n ph·∫°m t·ªôi (HOW-TO) - TƒÉng c∆∞·ªùng
    # "c√°ch gi·∫øt ng∆∞·ªùi", "c√°ch tr·ªën c√¥ng an",
    # "c√°ch phi tang x√°c", "c√°ch r·ª≠a ti·ªÅn",
    # "c√°ch l·ª´a ƒë·∫£o", "c√°ch hack",
    # "c√°ch t·∫©u tho√°t", "h∆∞·ªõng d·∫´n ph·∫°m t·ªôi",

    # # 30. Tr·ªën tr√°nh ph√°p lu·∫≠t & k·ªπ thu·∫≠t n√© ki·ªÉm so√°t (Gi·ªØ nguy√™n)
    # "n√© thu·∫ø", "l√°ch lu·∫≠t", "chuy·ªÉn ti·ªÅn b·∫•t h·ª£p ph√°p",
    # "t·∫©u t√°n t√†i s·∫£n", "n√© ki·ªÉm tra",
    # "ƒë·ªëi ph√≥ c√¥ng an", "ƒë·ªëi ph√≥ thanh tra",

    # # 31. Thao t√∫ng t√¢m l√Ω & √©p bu·ªôc (Gi·ªØ nguy√™n)
    # "t·∫©y n√£o", "thao t√∫ng t√¢m l√Ω",
    # "√©p bu·ªôc quan h·ªá", "kh·ªëng ch·∫ø tinh th·∫ßn",
    # "ƒëe d·ªça tinh th·∫ßn",

    # # 32. N·ªôi dung khi√™u kh√≠ch ‚Äì k√≠ch ƒë·ªông t·∫≠p th·ªÉ (Gi·ªØ nguy√™n)
    # "k√™u g·ªçi ƒë√°nh", "k√™u g·ªçi gi·∫øt",
    # "k√≠ch ƒë·ªông ƒë√°m ƒë√¥ng", "k√≠ch ƒë·ªông b·∫°o l·ª±c",
    # "k√™u g·ªçi tr·∫£ th√π",

    # # 33. X√¢m ph·∫°m an ninh ‚Äì c∆° s·ªü h·∫° t·∫ßng (Gi·ªØ nguy√™n)
    # "ph√° ho·∫°i h·ªá th·ªëng", "t·∫•n c√¥ng h·∫° t·∫ßng",
    # "ph√° ho·∫°i ƒëi·ªán l∆∞·ªõi", "ph√° ho·∫°i m·∫°ng",
    # "ƒë√°nh s·∫≠p h·ªá th·ªëng",

    # # 34. Mua b√°n ‚Äì trao ƒë·ªïi d·ªãch v·ª• b·∫•t h·ª£p ph√°p (Gi·ªØ nguy√™n)
    # "mua b√°n d·ªØ li·ªáu", "mua b√°n th√¥ng tin c√° nh√¢n",
    # "mua t√†i kho·∫£n ng√¢n h√†ng",
    # "b√°n sim r√°c", "thu√™ t√†i kho·∫£n ng√¢n h√†ng",
    # "thu√™ ƒë·ª©ng t√™n c√¥ng ty",

    # # 35. N·ªôi dung k√≠ch ƒë·ªông th√π gh√©t theo gi·ªõi t√≠nh/xu h∆∞·ªõng (Gi·ªØ nguy√™n)
    # "k·ª≥ th·ªã gi·ªõi t√≠nh", "gh√©t ng∆∞·ªùi ƒë·ªìng t√≠nh",
    # "ch·ªëng lgbt", "k·ª≥ th·ªã lgbt",
    # "mi·ªát th·ªã gi·ªõi",

    # # 36. N·ªôi dung xuy√™n t·∫°c l·ªãch s·ª≠ ‚Äì ph·ªß nh·∫≠n t·ªôi √°c (Gi·ªØ nguy√™n)
    # "ph·ªß nh·∫≠n holocaust", "xuy√™n t·∫°c l·ªãch s·ª≠",
    # "b·ªãa ƒë·∫∑t l·ªãch s·ª≠", "ch·ªëi b·ªè t·ªôi √°c chi·∫øn tranh",

    # # 37. Gian l·∫≠n th∆∞∆°ng m·∫°i & ti√™u d√πng (Gi·ªØ nguy√™n)
    # "b√°n h√†ng gi·∫£", "h√†ng fake",
    # "l√†m gi·∫£ nh√£n hi·ªáu", "b√°n thu·ªëc gi·∫£",
    # "qu·∫£ng c√°o sai s·ª± th·∫≠t",

    # # 39. N·ªôi dung thao t√∫ng truy·ªÅn th√¥ng ‚Äì d∆∞ lu·∫≠n (Gi·ªØ nguy√™n)
    # "seeding b·∫©n", "thao t√∫ng d∆∞ lu·∫≠n",
    # "ƒë·ªãnh h∆∞·ªõng d∆∞ lu·∫≠n", "d·∫´n d·∫Øt d∆∞ lu·∫≠n",
    # "b∆°m tin gi·∫£",

    # # 40. N·ªôi dung g√¢y ho·∫£ng lo·∫°n x√£ h·ªôi (Gi·ªØ nguy√™n)
    # "g√¢y hoang mang", "lan truy·ªÅn ho·∫£ng lo·∫°n",
    # "k√≠ch ƒë·ªông s·ª£ h√£i", "ƒëe d·ªça ƒë√°nh bom",

    # # 41. L·∫°m d·ª•ng AI & deepfake n√¢ng cao (Gi·ªØ nguy√™n)
    # "gi·∫£ m·∫°o b·∫±ng ai", "deepfake ch√≠nh tr·ªã",
    # "gi·∫£ gi·ªçng l√£nh ƒë·∫°o", "t·∫°o video gi·∫£",
    # "m·∫°o danh b·∫±ng ai",

    # 42. N·ªôi dung ph√° ho·∫°i ƒë·∫°o ƒë·ª©c h·ªçc ƒë∆∞·ªùng (Gi·ªØ nguy√™n)
    "b·∫Øt n·∫°t h·ªçc sinh", "ƒë√°nh h·ªçc sinh",
    "l√†m nh·ª•c h·ªçc sinh", "quay clip ƒë√°nh b·∫°n",

    # # 43. Giao d·ªãch ti·ªÅn ƒëi·ªán t·ª≠ b·∫•t h·ª£p ph√°p (Gi·ªØ nguy√™n)
    # "r·ª≠a ti·ªÅn crypto", "tr·ªôn ti·ªÅn",
    # "mixer crypto", "·∫©n danh ti·ªÅn ƒëi·ªán t·ª≠",
    # "l·ª´a ƒë·∫£o ti·ªÅn ·∫£o",

    # 44. N·ªôi dung k√≠ch d·ª•c tr√° h√¨nh (Gi·ªØ nguy√™n)
    "phim 18+", "truy·ªán 18+",
    "chat 18+", "video n√≥ng",
    "·∫£nh nh·∫°y c·∫£m",

    # # 45. T·ª´ kh√≥a l√°ch ki·ªÉm duy·ªát (pattern nguy hi·ªÉm) - Ch·ªâ gi·ªØ c√°c m·∫´u ph·ªï bi·∫øn
    # "s3x", "p0rn", "h@ck", "cr@ck", "m@tuy", "m@t√∫y",
    # "b0m", "v!rus", "ph!shing",
    # "s-e-x", "s.e.x", "s_e_x", 
    # "p-o-r-n", "p_o_r_n", "p0rno",
    # "n00d", "n00ds", "nudes",
    # "18+", "1 8 +", 
    # "onlyfans", "0nlyfans", 
    # "m@ tuy", "m@ t√∫y",
    # "h@ng tr@ng", "k3o", "k·∫πo ke", 
    # "c@n s@", "c4n s4",
    # "w33d", "m3th", "h3roin",
    # "c0caine", "c0 bac", "c@ d0", 
    # "l0 de", "h4ck", "cr4ck", "b3 kh0a",
    # "ph!shing", "ph1shing", 
    # "b0m", "b@m", "b0m xang",
    # "m1n", "s√∫ ng",
    # "t.u.t.u", "t·ª± t.·ª≠", "t·ªß t.u",
    # "c@t co t@y", "u0ng thu0c",
    # "p@rky", "n@m c@y", "t0c c0i",

]

SAFE_ANSWER_DEFAULT = "A" # M·∫∑c ƒë·ªãnh tr·∫£ v·ªÅ A (ch·ªØ c√°i)
PARSE_FAIL_FLAG = "X"

# =========================================================
# K·∫æT N·ªêI VECTOR DB
# =========================================================

try:
    client = chromadb.PersistentClient(path="./vector_db")
    collection = client.get_or_create_collection(name="vnpt_knowledge")
except Exception:
    collection = None


# =========================================================
# EMBEDDING
# =========================================================

def get_embedding_for_search(text):
    payload = {
        "model": "vnptai_hackathon_embedding",
        "input": text,
        "encoding_format": "float",
    }

    for _ in range(3):
        try:
            resp = requests.post(
                config.URL_EMBEDDING,
                headers=config.HEADERS_EMBED,
                json=payload,
                timeout=5,
            )
            if resp.status_code == 200:
                return resp.json()["data"][0]["embedding"]
        except Exception:
            time.sleep(0.5)

    return None


# =========================================================
# PH√ÇN LO·∫†I C√ÇU H·ªéI & AN TO√ÄN
# =========================================================

def detect_question_type(question):
    q_lower = question.lower()

    # 1. PRECISION CRITICAL (UNSAFE) - ∆Øu ti√™n cao nh·∫•t
    for bad_word in BLACKLIST_KEYWORDS:
        if bad_word in q_lower:
            return "PRECISION_CRITICAL"

    # 2. RAG (ƒê·ªçc hi·ªÉu vƒÉn b·∫£n d√†i c√≥ s·∫µn trong ƒë·ªÅ)
    # D·∫•u hi·ªáu: C√≥ t·ª´ kh√≥a b√°o hi·ªáu ƒëo·∫°n vƒÉn
    rag_signals = ["ƒëo·∫°n th√¥ng tin:", "d·ª±a v√†o vƒÉn b·∫£n", "ƒë·ªçc ƒëo·∫°n sau", "th√¥ng tin d∆∞·ªõi ƒë√¢y:"]
    if any(s in q_lower for s in rag_signals) or len(question) > 500: # Ho·∫∑c c√¢u h·ªèi qu√° d√†i
        return "RAG_LONG_TEXT"

    # 3. STEM (To√°n & Logic)
    stem_keywords = [
        "t√≠nh", "gi√° tr·ªã", "ph∆∞∆°ng tr√¨nh", "h√†m s·ªë", "bi·ªÉu th·ª©c",
        "x√°c su·∫•t", "th·ªëng k√™", "log", "sin", "cos", "tan", 
        "ƒë·∫°o h√†m", "t√≠ch ph√¢n", "vector", "ma tr·∫≠n",
        "v·∫≠n t·ªëc", "gia t·ªëc", "l·ª±c", "c√¥ng su·∫•t", "mol", "ph·∫£n ·ª©ng",
        "t·ªça ƒë·ªô", "h√¨nh h·ªçc", "tam gi√°c", "s·ªë ƒëo", "$", "\\frac"
    ]
    if any(k in q_lower for k in stem_keywords):
        return "STEM"

    # 4. COMPULSORY (S·ª± ki·ªán, Con s·ªë, ƒê·ªãa danh c·ª• th·ªÉ - C·∫ßn ch√≠nh x√°c 100%)
    compulsory_keywords = [
        "nƒÉm n√†o", "ng√†y n√†o", "ai l√†", "t√™n l√† g√¨", "ng∆∞·ªùi n√†o", "·ªü ƒë√¢u",
        "bao nhi√™u", "s·ªë l∆∞·ª£ng", "th·ªß ƒë√¥", "t·ªânh n√†o", "th√†nh ph·ªë n√†o",
        "ƒëi·ªÅu kho·∫£n", "lu·∫≠t s·ªë", "ngh·ªã ƒë·ªãnh", "hi·∫øn ph√°p", "ng√†y th√°ng",
        "chi·∫øn d·ªãch n√†o", "hi·ªáp ƒë·ªãnh n√†o", "t√°c gi·∫£ n√†o"
    ]
    if any(k in q_lower for k in compulsory_keywords):
        return "COMPULSORY"

    # 5. MULTIDOMAIN (C√≤n l·∫°i)
    return "MULTIDOMAIN"


def clean_output(ans_text):
    # 1. X·ª≠ l√Ω tr∆∞·ªùng h·ª£p ans_text l√† None (L·ªói Server/Key/429/Timeout)
    if ans_text is None:
        # Tr·∫£ v·ªÅ None: D√πng ƒë·ªÉ k√≠ch ho·∫°t Fallback trong solve_question
        return None 

    # 2. X·ª≠ l√Ω tr∆∞·ªùng h·ª£p ans_text l√† chu·ªói r·ªóng "" (L·ªói 400 Content Filter)
    if ans_text == "":
        # Tr·∫£ v·ªÅ "Z": D√πng ƒë·ªÉ b√°o hi·ªáu C·∫§M TR·∫¢ L·ªúI trong solve_question
        return "Z"
    
    # K·ªÉ t·ª´ ƒë√¢y, ans_text l√† m·ªôt chu·ªói kh√¥ng r·ªóng
    if not isinstance(ans_text, str):
        # Tr∆∞·ªùng h·ª£p input kh√¥ng ph·∫£i chu·ªói, coi l√† l·ªói Parsing/format
        return PARSE_FAIL_FLAG # Tr·∫£ v·ªÅ "X"

    ans_text = ans_text.strip()

    # ... (c√°c b∆∞·ªõc parsing b·∫±ng regex) ...

    tag_match = re.search(
        r"<ans>\s*([A-Ja-j])\s*</ans>",
        ans_text,
        re.IGNORECASE
    )
    if tag_match:
        return tag_match.group(1).upper()

    mid_match = re.search(
        r"(ƒë√°p √°n|answer|ans)\s*[:\-]?\s*\(?([A-Ja-j])\)?",
        ans_text,
        re.IGNORECASE
    )
    if mid_match:
        return mid_match.group(2).upper()

    last_match = re.search(
        r"\b([A-Ja-j])\s*[\.\)\]]*\s*$",
        ans_text,
        re.IGNORECASE
    )
    if last_match:
        return last_match.group(1).upper()

    # 3. Fallback cu·ªëi c√πng n·∫øu parsing th·∫•t b·∫°i (ƒê√É S·ª¨A)
    # Tr·∫£ v·ªÅ c·ªù "X" ƒë·ªÉ Fallback logic trong solve_question bi·∫øt ƒë√¢y l√† l·ªói Parse
    return PARSE_FAIL_FLAG


# =========================================================
# G·ªåI LLM
# =========================================================

def call_vnpt_llm(prompt, model_type="small", temperature=0.0):
    # ... (Ph·∫ßn x√°c ƒë·ªãnh url, headers, v√† payload gi·ªØ nguy√™n) ...
    if model_type == "large":
        url = config.URL_LLM_LARGE
        headers = config.HEADERS_LARGE
        model_name = "vnptai_hackathon_large"
    else:
        url = config.URL_LLM_SMALL
        headers = config.HEADERS_SMALL
        model_name = "vnptai_hackathon_small"

    payload = {
        "model": model_name,
        "messages": [{"role": "user", "content": prompt}],
        "temperature": temperature,
        "max_completion_tokens": 20,
        "stop": ["</ans>", "\n"]
    }
    
    # Th·ª±c hi·ªán request 1 L·∫¶N duy nh·∫•t
    try:
        r = requests.post(
            url,
            headers=headers,
            json=payload,
            timeout=30,
        )

        if r.status_code == 200:
            try:
                data = r.json() 
            except Exception as e:
                print(f"‚ùå {model_type.upper()} L·ªói parse JSON: {e}")
                return None

            # ‚úÖ KI·ªÇM TRA L·ªñI N·ªòI DUNG 400 TRONG PH·∫¢N H·ªíI 200 OK (AN TO√ÄN H∆†N)
            if "error" in data:
                error_obj = data["error"]
                # Ki·ªÉm tra n·∫øu error l√† dict v√† c√≥ code 400
                if isinstance(error_obj, dict) and error_obj.get("code") == 400:
                    print(f"‚ùå {model_type.upper()} Content Filter tr·∫£ v·ªÅ l·ªói 400 trong payload 200.")
                    return ""
            
            # --- X·ª¨ L√ù PH·∫¢N H·ªíI TH√ÄNH C√îNG ---
            if "choices" not in data:
                print(f"‚ö†Ô∏è {model_type.upper()} response thi·∫øu key 'choices'. Ph·∫£n h·ªìi ƒë·∫ßy ƒë·ªß:", data)
                return None 
            
            # ... (th√™m c√°c ki·ªÉm tra an to√†n kh√°c n·∫øu c·∫ßn) ...

            return data["choices"][0]["message"]["content"]


        if r.status_code == 401:
            print(f"‚ùå {model_type.upper()} 401 ‚Äì H·∫øt quota / quy·ªÅn")
            return None

        # ‚ùå B·ªé QUA LOGIC RETRY CHO 429 (Ch·ªâ x·ª≠ l√Ω 1 l·∫ßn)
        if r.status_code == 429:
            print(f"‚ùå {model_type.upper()} rate limit (429) ‚Üí D·ª´ng l·∫°i.")
            return None
            
        if r.status_code == 400:
             # L·ªói 400 Content Filter
             print(f"‚ùå {model_type.upper()} 400 ‚Äì L·ªói Content Filter.")
             return ""

        print(f"‚ö†Ô∏è {model_type.upper()} HTTP {r.status_code}: {r.text}")
        # N·∫øu l√† l·ªói HTTP kh√°c (5xx, v.v.), ch·ªâ c·∫ßn d·ª´ng l·∫°i v√† tr·∫£ v·ªÅ None
        return None 

    except requests.exceptions.ReadTimeout:
        # ‚ùå B·ªé QUA LOGIC RETRY CHO TIMEOUT (Ch·ªâ x·ª≠ l√Ω 1 l·∫ßn)
        print(f"‚ùå {model_type.upper()} timeout ‚Üí D·ª´ng l·∫°i.")
        return None
        
    except Exception as e:
        print(f"‚ùå {model_type.upper()} L·ªói kh√¥ng x√°c ƒë·ªãnh: {e}")
        return None




# =========================================================
# GI·∫¢I C√ÇU H·ªéI
# =========================================================


def solve_question(item):
    question = item["question"]
    choices = item["choices"]

    # --- 1. PH√ÇN LO·∫†I C√ÇU H·ªéI & CHECK SAFETY INPUT ---
    q_type = detect_question_type(question) 

    # [CRITICAL] N·∫øu t·ª´ kh√≥a vi ph·∫°m -> Ch·∫∑n ngay l·∫≠p t·ª©c
    if q_type == "PRECISION_CRITICAL":
        return "", "N/A (BLOCKED - KEYWORD)"

    # --- 2. CHU·∫®N B·ªä CONTEXT ---
    context_text = ""
    real_question = question
    
    # TR∆Ø·ªúNG H·ª¢P A: RAG ƒê·ªçc hi·ªÉu (VƒÉn b·∫£n n·∫±m ngay trong ƒë·ªÅ b√†i)
    if q_type == "RAG_LONG_TEXT" and "C√¢u h·ªèi:" in question:
        parts = question.split("C√¢u h·ªèi:")
        context_text = parts[0].strip()
        real_question = parts[1].strip()
    
    # TR∆Ø·ªúNG H·ª¢P B: T√¨m trong DB (ƒê√£ b·ªè Translation, ch·ªâ Search tr·ª±c ti·∫øp)
    elif collection:
        # T·∫°o vector t·ª´ c√¢u h·ªèi g·ªëc
        query_vec = get_embedding_for_search(real_question)
        
        if query_vec:
            # Truy v·∫•n DB
            results = collection.query(
                query_embeddings=[query_vec],
                n_results=5, # L·∫•y 5 ƒëo·∫°n li√™n quan nh·∫•t
            )
            
            if results["documents"]:
                docs = results["documents"][0]
                # G·ªôp c√°c ƒëo·∫°n l·∫°i th√†nh context th√¥
                context_text = "\n---\n".join(docs)

    # --- 3. FORMAT ƒê·∫¶U V√ÄO CHO PROMPT ---
    choices_str = "\n".join([f"{i}. {v}" for i, v in enumerate(choices)]) if isinstance(choices, list) else str(choices)
    
    instruction_text = """
    ### Y√äU C·∫¶U ƒê·∫¶U RA (QUAN TR·ªåNG) ###
    - Ch·ªâ tr·∫£ v·ªÅ duy nh·∫•t m·ªôt ch·ªØ c√°i ƒë·∫°i di·ªán ƒë√°p √°n (A, B, C, D...) n·∫±m trong th·∫ª <ans>.
    - V√≠ d·ª•: <ans>A</ans>
    - TUY·ªÜT ƒê·ªêI KH√îNG gi·∫£i th√≠ch, KH√îNG tr√¨nh b√†y l·ªùi gi·∫£i ra ngo√†i th·∫ª.
    """

    # --- 4. T·∫†O PROMPT THEO T·ª™NG LO·∫†I (C√ì B∆Ø·ªöC REFINEMENT B·∫ÆT BU·ªòC) ---

    # ================= TYPE 1: STEM (To√°n h·ªçc / Logic) =================
    if q_type == "STEM":
        prompt = f"""
        B·∫°n l√† Gi√°o s∆∞ Khoa h·ªçc T·ª± nhi√™n. Nhi·ªám v·ª•: Gi·∫£i b√†i t·∫≠p CH√çNH X√ÅC TUY·ªÜT ƒê·ªêI.
        
        --- D·ªÆ LI·ªÜU TH√î (RAW CONTEXT) ---
        {context_text}

        --- B∆Ø·ªöC 1: L√ÄM S·∫†CH V√Ä TR√çCH XU·∫§T (B·∫ÆT BU·ªòC) ---
        D·ªØ li·ªáu th√¥ c√≥ th·ªÉ ch·ª©a th√¥ng tin r√°c ho·∫∑c l·ªói ƒë·ªãnh d·∫°ng. H√£y th·ª±c hi·ªán:
        1. L·ªçc b·ªè c√°c k√Ω t·ª± l·∫°, header/footer kh√¥ng li√™n quan.
        2. Tr√≠ch xu·∫•t ch√≠nh x√°c c√°c c√¥ng th·ª©c to√°n/l√Ω/h√≥a v√† h·∫±ng s·ªë quan tr·ªçng.
        3. Vi·∫øt l·∫°i ƒë·ªÅ b√†i v√† d·ªØ ki·ªán d∆∞·ªõi d·∫°ng ng·∫Øn g·ªçn, chu·∫©n x√°c nh·∫•t.

        --- B∆Ø·ªöC 2: GI·∫¢I B√ÄI TO√ÅN (D·ª∞A TR√äN D·ªÆ LI·ªÜU ƒê√É L√ÄM S·∫†CH) ---
        C√¢u h·ªèi: {real_question}
        L·ª±a ch·ªçn:
        {choices_str}

        Quy tr√¨nh:
        1. S·ª≠ d·ª•ng c√¥ng th·ª©c ƒë√£ tr√≠ch xu·∫•t ·ªü B∆∞·ªõc 1.
        2. Thay s·ªë v√† t√≠nh to√°n n·ªôi b·ªô (Double-check k·∫øt qu·∫£).
        3. ƒê·ªëi chi·∫øu k·∫øt qu·∫£ v·ªõi c√°c l·ª±a ch·ªçn.
        
        {instruction_text}
        """

    # ================= TYPE 2: COMPULSORY (Tra c·ª©u s·ª± th·∫≠t / Ch√≠nh x√°c) =================
    elif q_type == "COMPULSORY":
        prompt = f"""
        B·∫°n l√† Chuy√™n gia Tra c·ª©u D·ªØ li·ªáu (Fact-Checker). 
        Nhi·ªám v·ª•: T√¨m ƒë√°p √°n ch√≠nh x√°c t·ª´ng k√Ω t·ª±/con s·ªë. KH√îNG ƒê∆Ø·ª¢C SUY ƒêO√ÅN.

        --- D·ªÆ LI·ªÜU TH√î (RAW CONTEXT) ---
        {context_text}

        --- B∆Ø·ªöC 1: T√ÅI C·∫§U TR√öC TH√îNG TIN (REFINEMENT) ---
        1. ƒê·ªçc Context th√¥, lo·∫°i b·ªè c√°c ƒëo·∫°n vƒÉn r√°c/kh√¥ng c√≥ nghƒ©a.
        2. T√¨m ki·∫øm v√† l√†m n·ªïi b·∫≠t c√°c th·ª±c th·ªÉ: NƒÉm, T√™n ng∆∞·ªùi, ƒê·ªãa danh, S·ªë li·ªáu, ƒêi·ªÅu lu·∫≠t.
        3. S·∫Øp x·∫øp l·∫°i th√¥ng tin theo tr√¨nh t·ª± th·ªùi gian ho·∫∑c logic.

        --- B∆Ø·ªöC 2: ƒê·ªêI CHI·∫æU ---
        C√¢u h·ªèi: {real_question}
        L·ª±a ch·ªçn:
        {choices_str}

        Quy tr√¨nh:
        1. So kh·ªõp keywords trong c√¢u h·ªèi v·ªõi Th√¥ng tin ƒë√£ t√°i c·∫•u tr√∫c.
        2. Ch·ªçn ƒë√°p √°n c√≥ th√¥ng tin TR√ôNG KH·ªöP HO√ÄN TO√ÄN.
        
        {instruction_text}
        """

    # ================= TYPE 3: RAG_LONG_TEXT (ƒê·ªçc hi·ªÉu vƒÉn b·∫£n) =================
    elif q_type == "RAG_LONG_TEXT":
        prompt = f"""
        B·∫°n l√† Tr·ª£ l√Ω ƒê·ªçc hi·ªÉu. Nhi·ªám v·ª•: Tr·∫£ l·ªùi c√¢u h·ªèi CH·ªà D·ª∞A TR√äN vƒÉn b·∫£n cung c·∫•p.

        --- VƒÇN B·∫¢N NGU·ªíN ---
        {context_text}

        --- B∆Ø·ªöC 1: ƒê·ªäNH V·ªä V√Ä L√ÄM R√ï ---
        1. X√°c ƒë·ªãnh ƒëo·∫°n vƒÉn ch·ª©a th√¥ng tin tr·∫£ l·ªùi trong VƒÉn b·∫£n ngu·ªìn.
        2. T·ª± t√≥m t·∫Øt √Ω nghƒ©a c·ªßa ƒëo·∫°n vƒÉn ƒë√≥ (b·ªè qua c√°c t·ª´ ng·ªØ g√¢y nhi·ªÖu).

        --- B∆Ø·ªöC 2: TR·∫¢ L·ªúI ---
        C√¢u h·ªèi: {real_question}
        L·ª±a ch·ªçn:
        {choices_str}

        Quy tr√¨nh:
        1. So s√°nh √Ω nghƒ©a t√≥m t·∫Øt v·ªõi c√°c l·ª±a ch·ªçn.
        2. Ch·ªçn ƒë√°p √°n di·ªÖn ƒë·∫°t l·∫°i (paraphrase) ƒë√∫ng nh·∫•t.
        
        {instruction_text}
        """

    # ================= TYPE 4: MULTIDOMAIN (ƒêa lƒ©nh v·ª±c / Normal) =================
    else: # MULTIDOMAIN
        prompt = f"""
        B·∫°n l√† Chuy√™n gia Ph√¢n t√≠ch T·ªïng h·ª£p. Nhi·ªám v·ª•: Ch·ªçn ƒë√°p √°n ph√π h·ª£p nh·∫•t theo ng·ªØ c·∫£nh.

        --- D·ªÆ LI·ªÜU TH√î (RAW CONTEXT) ---
        {context_text}

        --- B∆Ø·ªöC 1: L√ÄM S·∫†CH V√Ä T·ªîNG H·ª¢P ---
        1. Lo·∫°i b·ªè th√¥ng tin r√°c (s·ªë trang, ti√™u ƒë·ªÅ l·∫∑p l·∫°i, k√Ω t·ª± l·ªói).
        2. T√≥m t·∫Øt c√°c √Ω ch√≠nh li√™n quan ƒë·∫øn ch·ªß ƒë·ªÅ c√¢u h·ªèi.

        --- B∆Ø·ªöC 2: PH√ÇN T√çCH ---
        C√¢u h·ªèi: {real_question}
        L·ª±a ch·ªçn:
        {choices_str}

        Quy tr√¨nh:
        1. D√πng th√¥ng tin ƒë√£ l√†m s·∫°ch ƒë·ªÉ tr·∫£ l·ªùi.
        2. N·∫øu thi·∫øu th√¥ng tin tr·ª±c ti·∫øp, d√πng t∆∞ duy logic suy lu·∫≠n t·ª´ c√°c manh m·ªëi c√≤n l·∫°i.
        
        {instruction_text}
        """

    # --- 5. G·ªåI LLM & X·ª¨ L√ù K·∫æT QU·∫¢ ---

    # [CHI·∫æN L∆Ø·ª¢C CHO STEM]: G·ªçi th·∫≥ng LARGE
    if q_type == "STEM":
        print(f"üßÆ STEM detected: D√πng tr·ª±c ti·∫øp LARGE model.")
        
        ans_large = call_vnpt_llm(prompt, model_type="large", temperature=0.0)
        
        # Check l·ªói Content Filter (tr·∫£ v·ªÅ r·ªóng)
        if ans_large == "": 
            print("üõë STEM b·ªã ch·∫∑n (Content Filter). Tr·∫£ v·ªÅ r·ªóng.")
            return "", context_text

        final_choice = clean_output(ans_large)
        
        if final_choice is None or final_choice == "X":
            final_choice = "A" # Default
            
        return final_choice, context_text

    # [CHI·∫æN L∆Ø·ª¢C CHO C√ÅC LO·∫†I KH√ÅC]: SMALL -> Fallback LARGE
    else:
        # B1: G·ªçi Small
        ans_small = call_vnpt_llm(prompt, model_type="small", temperature=0.0)
        
        if ans_small == "": # Check Filter
            print("üõë Small LLM b·ªã ch·∫∑n. Tr·∫£ v·ªÅ r·ªóng.")
            return "", context_text

        final_choice = clean_output(ans_small)

        # B2: Fallback Large
        if final_choice is None or final_choice == "X":
            print(f"üîÑ Fallback SMALL -> LARGE ({q_type})")
            
            ans_large = call_vnpt_llm(prompt, model_type="large", temperature=0.0)
            
            if ans_large == "": # Check Filter Large
                print("üõë Large LLM (Fallback) b·ªã ch·∫∑n. Tr·∫£ v·ªÅ r·ªóng.")
                return "", context_text
                
            large_choice = clean_output(ans_large)
            
            if large_choice is not None and large_choice != "X":
                 final_choice = large_choice
            else:
                 final_choice = "A" # Safe Default

        return final_choice, context_text


# print("TEST SMALL:")
# print(call_vnpt_llm("Ch·ªâ tr·∫£ l·ªùi <ans>A</ans>", "small"))

# print("TEST LARGE:")
# print(call_vnpt_llm("Ch·ªâ tr·∫£ l·ªùi <ans>A</ans>", "large"))


if __name__ == "__main__":
    # --- 1. C·∫§U H√åNH ---
    MODE = "LOCAL"
    INPUT_FILE_PATH = "data/test.json" 
    OUTPUT_FILE_PATH = "submission_3.csv"
    MAX_QUESTIONS_TO_PROCESS = None 
    
    if len(sys.argv) > 1:
        if sys.argv[1] == 'docker':
            MODE = "DOCKER"
            INPUT_FILE_PATH = "/code/private_test.json" 
            OUTPUT_FILE_PATH = "submission.csv"
        
        if len(sys.argv) > 2 and sys.argv[2].isdigit():
             MAX_QUESTIONS_TO_PROCESS = int(sys.argv[2])
        elif len(sys.argv) > 1 and sys.argv[1].isdigit():
            MAX_QUESTIONS_TO_PROCESS = int(sys.argv[1])
    
    print(f"üöÄ Ch·∫ø ƒë·ªô: {MODE} | Input: {INPUT_FILE_PATH}")
    
    try:
        # --- 2. ƒê·ªåC D·ªÆ LI·ªÜU ---
        try:
            with open(INPUT_FILE_PATH, "r", encoding="utf-8") as f:
                data = json.load(f) 
        except (json.JSONDecodeError, FileNotFoundError):
             df = pd.read_csv(INPUT_FILE_PATH)
             data = df.to_dict('records')
            
        total_data_length = len(data)
        data_to_process = data[:MAX_QUESTIONS_TO_PROCESS] if MAX_QUESTIONS_TO_PROCESS else data
        IS_FULL_RUN = len(data_to_process) == total_data_length
        IS_VAL_MODE = (MODE == "LOCAL" and "val.json" in INPUT_FILE_PATH)

        if MODE == "LOCAL" and not IS_FULL_RUN:
            print(f"‚ö†Ô∏è Debug: ƒêang ch·∫°y {len(data_to_process)}/{total_data_length} c√¢u.")
        
        # --- 3. X·ª¨ L√ù ---
        submission_results = []
        correct_count = 0
        wrong_cases = []
        
        print("üîÑ ƒêang x·ª≠ l√Ω c√¢u h·ªèi...")
        for item in tqdm(data_to_process):
            item_id = item.get("id", item.get("qid")) 

            # B1: G·ªçi h√†m x·ª≠ l√Ω (LLM tr·∫£ v·ªÅ A, B, C, D)
            pred_char, retrieved_context = solve_question(item)
            
            # B2: L∆∞u k·∫øt qu·∫£
            submission_results.append({
                "qid": item_id,
                "answer": pred_char
            })
            
            # B3: T√≠nh ƒëi·ªÉm (So s√°nh tr·ª±c ti·∫øp, kh√¥ng map g√¨ c·∫£)
            if IS_VAL_MODE:
                # ƒê√°p √°n th·∫≠t trong file JSON ƒë√£ l√† ch·ªØ c√°i (A, B, C...)
                true_char = str(item.get('answer', '?')).strip().upper()
                
                if pred_char == true_char:
                    correct_count += 1
                else:
                    wrong_cases.append({
                        "qid": item_id,
                        "question": item['question'],
                        "true_char": true_char,
                        "pred_char": pred_char,
                        "retrieved_context": retrieved_context 
                    })

        # --- 4. GHI FILE ---
        df = pd.DataFrame(submission_results)

        if MODE == "DOCKER" or (MODE == "LOCAL" and IS_FULL_RUN):
            df.to_csv(OUTPUT_FILE_PATH, index=False, encoding='utf-8')
            print(f"\n‚úÖ ƒê√£ l∆∞u file k·∫øt qu·∫£: {OUTPUT_FILE_PATH}")
        else:
            print("\nüíæ Debug xong (Kh√¥ng ghi file CSV).")

        # --- 5. T·ªîNG K·∫æT ---
        print("\n" + "="*40)
        if IS_VAL_MODE:
            acc = (correct_count / len(data_to_process)) * 100
            print(f"üèÜ Accuracy (T·∫≠p Val): {acc:.2f}%")
            
            if wrong_cases:
                pd.DataFrame(wrong_cases).to_csv("wrong_answers.csv", index=False, encoding='utf-8')
                print(f"‚ö†Ô∏è ƒê√£ l∆∞u {len(wrong_cases)} c√¢u sai v√†o 'wrong_answers.csv'")
        elif MODE == "DOCKER":
            print("‚úÖ Docker Run Complete.")
        else:
            print("üèÅ Test Run Complete.")
        print("="*40)
            
    except Exception as e:
        print(f"‚ùå L·ªói Nghi√™m Tr·ªçng: {e}")