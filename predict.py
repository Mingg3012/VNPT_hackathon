import json
import re
import time
import requests
import chromadb
from tqdm import tqdm
import config # File config c·ªßa b·∫°n
import sys
import pandas as pd

try:
    __import__('pysqlite3')
    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
except ImportError:
    pass

# --- C·∫§U H√åNH ---
BLACKLIST_KEYWORDS = ["sex", "khi√™u d√¢m"] 
SAFE_ANSWER_DEFAULT = "A" # M·∫∑c ƒë·ªãnh tr·∫£ v·ªÅ A (ch·ªØ c√°i)

# =========================================================
# K·∫æT N·ªêI VECTOR DB
# =========================================================

try:
    client = chromadb.PersistentClient(path="./vector_db")
    collection = client.get_or_create_collection(name="vnpt_knowledge")
except Exception:
    collection = None


# =========================================================
# EMBEDDING
# =========================================================

def get_embedding_for_search(text):
    payload = {
        "model": "vnptai_hackathon_embedding",
        "input": text,
        "encoding_format": "float",
    }

    for _ in range(3):
        try:
            resp = requests.post(
                config.URL_EMBEDDING,
                headers=config.HEADERS_EMBED,
                json=payload,
                timeout=5,
            )
            if resp.status_code == 200:
                return resp.json()["data"][0]["embedding"]
        except Exception:
            time.sleep(0.5)

    return None


# =========================================================
# PH√ÇN LO·∫†I C√ÇU H·ªéI & AN TO√ÄN
# =========================================================

def detect_question_type_and_safety(question):
    q_lower = question.lower()

    for bad_word in BLACKLIST_KEYWORDS:
        if bad_word in q_lower:
            return "UNSAFE"

    stem_keywords = [
        "t√≠nh", "gi√° tr·ªã", "ph∆∞∆°ng tr√¨nh", "h√†m s·ªë", "bi·ªÉu th·ª©c",
        "x√°c su·∫•t", "th·ªëng k√™", "log", "sin", "cos", "tan", "cot",
        "ƒë·∫°o h√†m", "t√≠ch ph√¢n", "nguy√™n h√†m", "vector", "ma tr·∫≠n",
        "v·∫≠n t·ªëc", "gia t·ªëc", "l·ª±c", "ƒëi·ªán tr·ªü", "nƒÉng l∆∞·ª£ng", "c√¥ng su·∫•t",
        "l√£i su·∫•t", "gdp", "l·∫°m ph√°t", "cung c·∫ßu", "ƒë·ªô co gi√£n",
        "mol", "ph·∫£n ·ª©ng", "c√¢n b·∫±ng", "kh·ªëi l∆∞·ª£ng",
    ]

    if any(k in q_lower for k in stem_keywords):
        return "STEM"

    precision_keywords = [
        "nƒÉm n√†o", "ng√†y n√†o", "ai l√†", "ng∆∞·ªùi n√†o", "·ªü ƒë√¢u",
        "bao nhi√™u", "s·ªë l∆∞·ª£ng", "th·ªùi gian n√†o",
        "ngh·ªã ƒë·ªãnh", "lu·∫≠t", "th√¥ng t∆∞", "ƒëi·ªÅu kho·∫£n", "hi·∫øn ph√°p",
        "th·ªß ƒë√¥", "di t√≠ch", "chi·∫øn d·ªãch", "hi·ªáp ƒë·ªãnh",
    ]

    if any(k in q_lower for k in precision_keywords):
        return "PRECISION"

    return "NORMAL"


# =========================================================
# CLEAN OUTPUT (B·∫ÆT CH·ªÆ C√ÅI)
# =========================================================

def clean_output(ans_text):
    # Regex t√¨m k√Ω t·ª± A, B, C, D (kh√¥ng ph√¢n bi·ªát hoa th∆∞·ªùng)
    tag_match = re.search(r"<ans>\s*([A-Da-d])\s*</ans>", ans_text)
    if tag_match:
        return tag_match.group(1).upper()

    matches = re.findall(r"\b([A-Da-d])\b", ans_text)
    if matches:
        return matches[-1].upper()

    return SAFE_ANSWER_DEFAULT


# =========================================================
# G·ªåI LLM
# =========================================================

def call_vnpt_llm(prompt, model_type="small"):
    if model_type == "large":
        url = config.URL_LLM_LARGE
        headers = config.HEADERS_LARGE
        model = "vnptai_hackathon_large"
        max_tokens = 400
    else:
        url = config.URL_LLM_SMALL
        headers = config.HEADERS_SMALL
        model = "vnptai_hackathon_small"
        max_tokens = 150

    payload = {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.1,
        "max_completion_tokens": max_tokens,
    }

    for _ in range(5):
        try:
            r = requests.post(url, headers=headers, json=payload, timeout=40)
            if r.status_code == 200:
                return r.json()["choices"][0]["message"]["content"]
            elif r.status_code == 429:
                print("‚è≥ Rate limit, ng·ªß 60s...")
                time.sleep(60)
            else:
                time.sleep(2)
        except Exception:
            time.sleep(2)

    return ""


# =========================================================
# GI·∫¢I C√ÇU H·ªéI
# =========================================================

def solve_question(item):
    question = item["question"]
    choices = item["choices"]

    q_type = detect_question_type_and_safety(question)

    if q_type == "UNSAFE":
        return SAFE_ANSWER_DEFAULT, "N/A (UNSAFE QUESTION/FILTERED)"

    context_text = ""
    real_question = question
    is_reading_comprehension = "ƒêo·∫°n th√¥ng tin:" in question

    if is_reading_comprehension:
        parts = question.split("C√¢u h·ªèi:")
        context_text = parts[0].strip()
        real_question = parts[1].strip() if len(parts) > 1 else question
    elif collection:
        query_vec = get_embedding_for_search(real_question)
        if query_vec:
            results = collection.query(
                query_embeddings=[query_vec],
                n_results=5,
            )
            if results["documents"]:
                docs = results["documents"][0]
                context_text = "\n---\n".join(docs)

    # --- GI·ªÆ NGUY√äN format choices (0. ..., 1. ...) cho ƒë·ª° m·∫•t c√¥ng ---
    choices_str = (
        "\n. ".join([f"{i}. {v}" for i, v in enumerate(choices)])
        if isinstance(choices, list)
        else str(choices)
    )

    CONTEXT_LENGTH_THRESHOLD = 1200
    model_to_use = "small"

    # --- Prompt ch·ªâ th·ªã LLM tr·∫£ v·ªÅ ch·ªØ c√°i t∆∞∆°ng ·ª©ng ---
    instruction_text = "H√£y ch·ªçn ƒë√°p √°n ƒë√∫ng (t∆∞∆°ng ·ª©ng 0->A, 1->B, 2->C, 3->D, 3->D, 4->E, 5->F, 6->G, 7->H, 8->I, 9->J) v√† ch·ªâ tr·∫£ v·ªÅ ch·ªØ c√°i (A, B, C, D) trong th·∫ª <ans>."

    if q_type == "STEM":
        model_to_use = "large"
        prompt = f"""
        B·∫°n l√† m·ªôt Gi√°o s∆∞ Khoa h·ªçc T·ª± nhi√™n xu·∫•t s·∫Øc.

        --- TH√îNG TIN B·ªî TR·ª¢ ---
        {context_text}

        C√¢u h·ªèi:
        {real_question}

        C√°c l·ª±a ch·ªçn (ƒë√°nh s·ªë t·ª´ 0):
        {choices_str}

        CHI·∫æN L∆Ø·ª¢C SUY LU·∫¨N:
        1. X√°c ƒë·ªãnh d·∫°ng b√†i v√† c√¥ng th·ª©c/ƒë·ªãnh lu·∫≠t c·∫ßn d√πng.
        2. Tr√≠ch xu·∫•t d·ªØ ki·ªán quan tr·ªçng.
        3. Th·ª±c hi·ªán t√≠nh to√°n ho·∫∑c suy lu·∫≠n logic.
        4. SO KH·ªöP k·∫øt qu·∫£ v·ªõi T·∫§T C·∫¢ c√°c l·ª±a ch·ªçn v√† lo·∫°i tr·ª´ c√°c ph∆∞∆°ng √°n sai.
        5. Ch·ªçn ph∆∞∆°ng √°n DUY NH·∫§T ƒë√∫ng nh·∫•t.

        Y√äU C·∫¶U ƒê·∫¶U RA:
        - Tr√¨nh b√†y suy lu·∫≠n ng·∫Øn g·ªçn.
        - {instruction_text}
        """
    else:
        if (
            len(context_text) > CONTEXT_LENGTH_THRESHOLD
            or q_type == "PRECISION"
            or is_reading_comprehension
        ):
            model_to_use = "large"

        prompt = f"""
        B·∫°n l√† chuy√™n gia ph√¢n t√≠ch th√¥ng tin.

        --- D·ªÆ LI·ªÜU THAM KH·∫¢O (CONTEXT) ---
        {context_text}
        --- H·∫æT CONTEXT ---

        C√¢u h·ªèi:
        {real_question}

        C√°c l·ª±a ch·ªçn (ƒë√°nh s·ªë t·ª´ 0):
        {choices_str}

        NGUY√äN T·∫ÆC QUAN TR·ªåNG:
        - N·∫øu CONTEXT c√≥ th√¥ng tin li√™n quan tr·ª±c ti·∫øp: PH·∫¢I ∆∞u ti√™n CONTEXT.
        - Ch·ªâ d√πng ki·∫øn th·ª©c b√™n ngo√†i khi CONTEXT kh√¥ng ƒë·ªß ho·∫∑c kh√¥ng li√™n quan.
        - Kh√¥ng ƒë∆∞·ª£c t·ª± suy di·ªÖn tr√°i v·ªõi CONTEXT.

        CHI·∫æN L∆Ø·ª¢C:
        1. T√¨m c√¢u trong CONTEXT li√™n quan tr·ª±c ti·∫øp ƒë·∫øn c√¢u h·ªèi.
        2. ƒê·ªëi chi·∫øu t·ª´ng l·ª±a ch·ªçn v·ªõi CONTEXT.
        3. Lo·∫°i tr·ª´ c√°c ph∆∞∆°ng √°n kh√¥ng kh·ªõp.
        4. Ch·ªçn ƒë√°p √°n ƒë√∫ng nh·∫•t.

        Y√äU C·∫¶U ƒê·∫¶U RA:
        {instruction_text}
        """

    ans = call_vnpt_llm(prompt, model_type=model_to_use)
    final_choice = clean_output(ans)

    return final_choice, context_text


if __name__ == "__main__":
    # --- 1. C·∫§U H√åNH ---
    MODE = "LOCAL"
    INPUT_FILE_PATH = "data/val.json" 
    OUTPUT_FILE_PATH = "submission_local.csv"
    MAX_QUESTIONS_TO_PROCESS = None 
    
    if len(sys.argv) > 1:
        if sys.argv[1] == 'docker':
            MODE = "DOCKER"
            INPUT_FILE_PATH = "/code/private_test.json" 
            OUTPUT_FILE_PATH = "submission.csv"
        
        if len(sys.argv) > 2 and sys.argv[2].isdigit():
             MAX_QUESTIONS_TO_PROCESS = int(sys.argv[2])
        elif len(sys.argv) > 1 and sys.argv[1].isdigit():
            MAX_QUESTIONS_TO_PROCESS = int(sys.argv[1])
    
    print(f"üöÄ Ch·∫ø ƒë·ªô: {MODE} | Input: {INPUT_FILE_PATH}")
    
    try:
        # --- 2. ƒê·ªåC D·ªÆ LI·ªÜU ---
        try:
            with open(INPUT_FILE_PATH, "r", encoding="utf-8") as f:
                data = json.load(f) 
        except (json.JSONDecodeError, FileNotFoundError):
             df = pd.read_csv(INPUT_FILE_PATH)
             data = df.to_dict('records')
            
        total_data_length = len(data)
        data_to_process = data[:MAX_QUESTIONS_TO_PROCESS] if MAX_QUESTIONS_TO_PROCESS else data
        IS_FULL_RUN = len(data_to_process) == total_data_length
        IS_VAL_MODE = (MODE == "LOCAL" and "val.json" in INPUT_FILE_PATH)

        if MODE == "LOCAL" and not IS_FULL_RUN:
            print(f"‚ö†Ô∏è Debug: ƒêang ch·∫°y {len(data_to_process)}/{total_data_length} c√¢u.")
        
        # --- 3. X·ª¨ L√ù ---
        submission_results = []
        correct_count = 0
        wrong_cases = []
        
        print("üîÑ ƒêang x·ª≠ l√Ω c√¢u h·ªèi...")
        for item in tqdm(data_to_process):
            item_id = item.get("id", item.get("qid")) 

            # B1: G·ªçi h√†m x·ª≠ l√Ω (LLM tr·∫£ v·ªÅ A, B, C, D)
            pred_char, retrieved_context = solve_question(item)
            
            # B2: L∆∞u k·∫øt qu·∫£
            submission_results.append({
                "qid": item_id,
                "answer": pred_char
            })
            
            # B3: T√≠nh ƒëi·ªÉm (So s√°nh tr·ª±c ti·∫øp, kh√¥ng map g√¨ c·∫£)
            if IS_VAL_MODE:
                # ƒê√°p √°n th·∫≠t trong file JSON ƒë√£ l√† ch·ªØ c√°i (A, B, C...)
                true_char = str(item.get('answer', '?')).strip().upper()
                
                if pred_char == true_char:
                    correct_count += 1
                else:
                    wrong_cases.append({
                        "qid": item_id,
                        "question": item['question'],
                        "true_char": true_char,
                        "pred_char": pred_char,
                        "retrieved_context": retrieved_context 
                    })

        # --- 4. GHI FILE ---
        df = pd.DataFrame(submission_results)

        if MODE == "DOCKER" or (MODE == "LOCAL" and IS_FULL_RUN):
            df.to_csv(OUTPUT_FILE_PATH, index=False, encoding='utf-8')
            print(f"\n‚úÖ ƒê√£ l∆∞u file k·∫øt qu·∫£: {OUTPUT_FILE_PATH}")
        else:
            print("\nüíæ Debug xong (Kh√¥ng ghi file CSV).")

        # --- 5. T·ªîNG K·∫æT ---
        print("\n" + "="*40)
        if IS_VAL_MODE:
            acc = (correct_count / len(data_to_process)) * 100
            print(f"üèÜ Accuracy (T·∫≠p Val): {acc:.2f}%")
            
            if wrong_cases:
                pd.DataFrame(wrong_cases).to_csv("wrong_answers.csv", index=False, encoding='utf-8')
                print(f"‚ö†Ô∏è ƒê√£ l∆∞u {len(wrong_cases)} c√¢u sai v√†o 'wrong_answers.csv'")
        elif MODE == "DOCKER":
            print("‚úÖ Docker Run Complete.")
        else:
            print("üèÅ Test Run Complete.")
        print("="*40)
            
    except Exception as e:
        print(f"‚ùå L·ªói Nghi√™m Tr·ªçng: {e}")