import json
import re
import time
import requests
import chromadb
from tqdm import tqdm
import config # File config c·ªßa b·∫°n
import sys
import pandas as pd

try:
    __import__('pysqlite3')
    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
except ImportError:
    pass

# --- C·∫§U H√åNH ---
BLACKLIST_KEYWORDS = ["sex", "khi√™u d√¢m"] 
SAFE_ANSWER_DEFAULT = "A" # M·∫∑c ƒë·ªãnh tr·∫£ v·ªÅ A (ch·ªØ c√°i)

# =========================================================
# K·∫æT N·ªêI VECTOR DB
# =========================================================

try:
    client = chromadb.PersistentClient(path="./vector_db")
    collection = client.get_or_create_collection(name="vnpt_knowledge")
except Exception:
    collection = None


# =========================================================
# EMBEDDING
# =========================================================

def get_embedding_for_search(text):
    payload = {
        "model": "vnptai_hackathon_embedding",
        "input": text,
        "encoding_format": "float",
    }

    for _ in range(3):
        try:
            resp = requests.post(
                config.URL_EMBEDDING,
                headers=config.HEADERS_EMBED,
                json=payload,
                timeout=5,
            )
            if resp.status_code == 200:
                return resp.json()["data"][0]["embedding"]
        except Exception:
            time.sleep(0.5)

    return None


# =========================================================
# PH√ÇN LO·∫†I C√ÇU H·ªéI & AN TO√ÄN
# =========================================================

def detect_question_type_and_safety(question):
    q_lower = question.lower()

    for bad_word in BLACKLIST_KEYWORDS:
        if bad_word in q_lower:
            return "UNSAFE"

    stem_keywords = [
        "t√≠nh", "gi√° tr·ªã", "ph∆∞∆°ng tr√¨nh", "h√†m s·ªë", "bi·ªÉu th·ª©c",
        "x√°c su·∫•t", "th·ªëng k√™", "log", "sin", "cos", "tan", "cot",
        "ƒë·∫°o h√†m", "t√≠ch ph√¢n", "nguy√™n h√†m", "vector", "ma tr·∫≠n",
        "v·∫≠n t·ªëc", "gia t·ªëc", "l·ª±c", "ƒëi·ªán tr·ªü", "nƒÉng l∆∞·ª£ng", "c√¥ng su·∫•t",
        "l√£i su·∫•t", "gdp", "l·∫°m ph√°t", "cung c·∫ßu", "ƒë·ªô co gi√£n",
        "mol", "ph·∫£n ·ª©ng", "c√¢n b·∫±ng", "kh·ªëi l∆∞·ª£ng", "latex", "$", "\\frac" 
        ]

    if any(k in q_lower for k in stem_keywords):
        return "STEM"

    precision_keywords = [
        "nƒÉm n√†o", "ng√†y n√†o", "ai l√†", "ng∆∞·ªùi n√†o", "·ªü ƒë√¢u",
        "bao nhi√™u", "s·ªë l∆∞·ª£ng", "th·ªùi gian n√†o",
        "ngh·ªã ƒë·ªãnh", "lu·∫≠t", "th√¥ng t∆∞", "ƒëi·ªÅu kho·∫£n", "hi·∫øn ph√°p",
        "th·ªß ƒë√¥", "di t√≠ch", "chi·∫øn d·ªãch", "hi·ªáp ƒë·ªãnh",
    ]

    if any(k in q_lower for k in precision_keywords):
        return "PRECISION"

    return "NORMAL"


def clean_output(ans_text):
    # 1. ∆Øu ti√™n tuy·ªát ƒë·ªëi: T√¨m trong th·∫ª <ans>
    # B·∫Øt A-J, kh√¥ng ph√¢n bi·ªát hoa th∆∞·ªùng
    tag_match = re.search(r"<ans>\s*([A-Ja-j])\s*</ans>", ans_text, re.IGNORECASE)
    if tag_match:
        return tag_match.group(1).upper()

    # 2. N·∫øu kh√¥ng c√≥ th·∫ª, ch·ªâ t√¨m ch·ªØ c√°i ƒë·ª©ng ri√™ng l·∫ª ·ªû CU·ªêI C√ôNG c·ªßa chu·ªói output
    # Regex n√†y ch·ªâ b·∫Øt A-J n·∫øu n√≥ n·∫±m ·ªü cu·ªëi c√¢u (c√≥ th·ªÉ theo sau l√† d·∫•u ch·∫•m/xu·ªëng d√≤ng)
    # Tr√°nh b·∫Øt nh·∫ßm ch·ªØ "a" trong "gia t·ªëc a" n·∫±m ·ªü gi·ªØa c√¢u.
    last_match = re.search(r"\b([A-Ja-j])\s*(\.|)\s*$", ans_text, re.IGNORECASE)
    if last_match:
        return last_match.group(1).upper()
        
    # 3. Fallback: N·∫øu v·∫´n kh√¥ng t√¨m th·∫•y, c√≥ th·ªÉ tr·∫£ v·ªÅ None ƒë·ªÉ debug ho·∫∑c ch·ªçn ƒë·∫°i A
    # Khuy√™n d√πng: In ra c·∫£nh b√°o ƒë·ªÉ bi·∫øt model ƒëang kh√¥ng tu√¢n th·ªß format
    # print(f"‚ö†Ô∏è C·∫£nh b√°o: Kh√¥ng t√¨m th·∫•y ƒë√°p √°n trong output: {ans_text[:50]}...")
    return SAFE_ANSWER_DEFAULT
# =========================================================
# G·ªåI LLM
# =========================================================

def call_vnpt_llm(prompt, model_type="small", temperature=0.1):
    if model_type == "large":
        url = config.URL_LLM_LARGE
        headers = config.HEADERS_LARGE
        model = "vnptai_hackathon_large"
        max_tokens = 400
    else:
        url = config.URL_LLM_SMALL
        headers = config.HEADERS_SMALL
        model = "vnptai_hackathon_small"
        max_tokens = 150

    payload = {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "temperature": temperature,
        "max_completion_tokens": max_tokens,
    }

    for _ in range(5):
        try:
            r = requests.post(url, headers=headers, json=payload, timeout=40)
            if r.status_code == 200:
                return r.json()["choices"][0]["message"]["content"]
            elif r.status_code == 429:
                print("‚è≥ Rate limit, ng·ªß 60s...")
                time.sleep(60)
            else:
                time.sleep(2)
        except Exception:
            time.sleep(2)

    return ""


# =========================================================
# GI·∫¢I C√ÇU H·ªéI
# =========================================================

def solve_question(item):
    question = item["question"]
    choices = item["choices"]

    q_type = detect_question_type_and_safety(question)

    if q_type == "UNSAFE":
        return SAFE_ANSWER_DEFAULT, "N/A (UNSAFE QUESTION/FILTERED)"

    context_text = ""
    real_question = question
    is_reading_comprehension = "ƒêo·∫°n th√¥ng tin:" in question

    if is_reading_comprehension:
        parts = question.split("C√¢u h·ªèi:")
        context_text = parts[0].strip()
        real_question = parts[1].strip() if len(parts) > 1 else question
    elif collection:
        query_vec = get_embedding_for_search(real_question)
        if query_vec:
            results = collection.query(
                query_embeddings=[query_vec],
                n_results=5,
            )
            if results["documents"]:
                docs = results["documents"][0]
                context_text = "\n---\n".join(docs)

    # --- GI·ªÆ NGUY√äN format choices (0. ..., 1. ...) cho ƒë·ª° m·∫•t c√¥ng ---
    choices_str = (
        "\n. ".join([f"{i}. {v}" for i, v in enumerate(choices)])
        if isinstance(choices, list)
        else str(choices)
    )
    model_to_use = "small"

    # --- Prompt ch·ªâ th·ªã LLM tr·∫£ v·ªÅ ch·ªØ c√°i t∆∞∆°ng ·ª©ng ---
    instruction_text = "H√£y ch·ªçn ƒë√°p √°n ƒë√∫ng (t∆∞∆°ng ·ª©ng 0->A, 1->B, 2->C, 3->D, 4->E, 5->F, 6->G, 7->H, 8->I, 9->J) v√† ch·ªâ tr·∫£ v·ªÅ ch·ªØ c√°i (A, B, C, D, E, F, G, H, I, J). B·∫ÆT BU·ªòC: ƒê√°p √°n cu·ªëi c√πng ph·∫£i n·∫±m trong th·∫ª <ans>, v√≠ d·ª•: <ans>A</ans>."

    if q_type == "STEM":
        model_to_use = "large"
        prompt = f"""
        B·∫°n l√† Gi√°o s∆∞ Khoa h·ªçc T·ª± nhi√™n. Nhi·ªám v·ª•: Gi·∫£i b√†i t·∫≠p ch√≠nh x√°c tuy·ªát ƒë·ªëi.

        --- C√îNG TH·ª®C & KI·∫æN TH·ª®C B·ªî TR·ª¢ (CONTEXT) ---
        {context_text}

        --- B√ÄI TO√ÅN ---
        C√¢u h·ªèi: {real_question}

        C√°c l·ª±a ch·ªçn (Index t·ª´ 0):
        {choices_str}

        --- H∆Ø·ªöNG D·∫™N GI·∫¢I ---
        1. X√°c ƒë·ªãnh c√¥ng th·ª©c/ƒë·ªãnh l√Ω t·ª´ CONTEXT c·∫ßn d√πng.
        2. Tr√≠ch xu·∫•t c√°c con s·ªë t·ª´ C√¢u h·ªèi (L∆∞u √Ω ƒë∆°n v·ªã).
        3. Th·ª±c hi·ªán t√≠nh to√°n n·ªôi b·ªô (KH√îNG tr√¨nh b√†y ra ngo√†i) nh∆∞ng kh√¥ng ƒë∆∞·ª£c ƒëo√°n m√≤.
        4. Ch·ªâ ch·ªçn M·ªòT ƒë√°p √°n duy nh·∫•t kh·ªõp k·∫øt qu·∫£.

        --- Y√äU C·∫¶U ƒê·∫¶U RA (B·∫ÆT BU·ªòC) ---
        - KH√îNG tr√¨nh b√†y l·ªùi gi·∫£i.
        - KH√îNG gi·∫£i th√≠ch d√†i.
        - {instruction_text}
        """
    else:
        # Prompt Context nh·∫•n m·∫°nh v√†o vi·ªác trung th√†nh v·ªõi vƒÉn b·∫£n
        CONTEXT_LENGTH_THRESHOLD = 1200
        if len(context_text) > CONTEXT_LENGTH_THRESHOLD or q_type == "PRECISION" or "ƒêo·∫°n th√¥ng tin:" in question:
            model_to_use = "large"

        prompt = f"""
        B·∫°n l√† chuy√™n gia ph√¢n t√≠ch th√¥ng tin. Nhi·ªám v·ª•: H√£y ƒë·ªçc th·∫≠t kƒ© vƒÉn b·∫£n v√† tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n vƒÉn b·∫£n cung c·∫•p.

        --- VƒÇN B·∫¢N THAM KH·∫¢O (CONTEXT) ---
        {context_text}

        --- C√ÇU H·ªéI ---
        {real_question}

        C√°c l·ª±a ch·ªçn:
        {choices_str}

        --- CHI·∫æN L∆Ø·ª¢C ---
        1. T√¨m th√¥ng tin trong CONTEXT kh·ªõp v·ªõi t·ª´ kh√≥a c√¢u h·ªèi.
        2. Ch·ªçn ƒë√°p √°n ƒê∆Ø·ª¢C H·ªñ TR·ª¢ B·ªûI CONTEXT.
        3. N·∫øu CONTEXT kh√¥ng ƒë·ªß th√¥ng tin, ch·ªçn ƒë√°p √°n ƒë∆∞·ª£c nh·∫Øc tr·ª±c ti·∫øp ho·∫∑c suy ra r√µ r√†ng nh·∫•t t·ª´ CONTEXT. Kh√¥ng suy ƒëo√°n ngo√†i.
        4. N·∫øu c√¢u h·ªèi y√™u c·∫ßu suy lu·∫≠n logic, h√£y th·ª≠ suy lu·∫≠n.

        --- Y√äU C·∫¶U ƒê·∫¶U RA (B·∫ÆT BU·ªòC) ---
        - KH√îNG gi·∫£i th√≠ch d√†i, lan man.
        - {instruction_text}
        """

    if q_type == "STEM":
        ans = call_vnpt_llm(prompt, model_type="large", temperature=0.0)
    else:
        ans = call_vnpt_llm(prompt, model_type=model_to_use, temperature=0.1)

    final_choice = clean_output(ans)
    return final_choice, context_text


if __name__ == "__main__":
    # --- 1. C·∫§U H√åNH ---
    MODE = "LOCAL"
    INPUT_FILE_PATH = "data/val.json" 
    OUTPUT_FILE_PATH = "submission_local.csv"
    MAX_QUESTIONS_TO_PROCESS = None 
    
    if len(sys.argv) > 1:
        if sys.argv[1] == 'docker':
            MODE = "DOCKER"
            INPUT_FILE_PATH = "/code/private_test.json" 
            OUTPUT_FILE_PATH = "submission.csv"
        
        if len(sys.argv) > 2 and sys.argv[2].isdigit():
             MAX_QUESTIONS_TO_PROCESS = int(sys.argv[2])
        elif len(sys.argv) > 1 and sys.argv[1].isdigit():
            MAX_QUESTIONS_TO_PROCESS = int(sys.argv[1])
    
    print(f"üöÄ Ch·∫ø ƒë·ªô: {MODE} | Input: {INPUT_FILE_PATH}")
    
    try:
        # --- 2. ƒê·ªåC D·ªÆ LI·ªÜU ---
        try:
            with open(INPUT_FILE_PATH, "r", encoding="utf-8") as f:
                data = json.load(f) 
        except (json.JSONDecodeError, FileNotFoundError):
             df = pd.read_csv(INPUT_FILE_PATH)
             data = df.to_dict('records')
            
        total_data_length = len(data)
        data_to_process = data[:MAX_QUESTIONS_TO_PROCESS] if MAX_QUESTIONS_TO_PROCESS else data
        IS_FULL_RUN = len(data_to_process) == total_data_length
        IS_VAL_MODE = (MODE == "LOCAL" and "val.json" in INPUT_FILE_PATH)

        if MODE == "LOCAL" and not IS_FULL_RUN:
            print(f"‚ö†Ô∏è Debug: ƒêang ch·∫°y {len(data_to_process)}/{total_data_length} c√¢u.")
        
        # --- 3. X·ª¨ L√ù ---
        submission_results = []
        correct_count = 0
        wrong_cases = []
        
        print("üîÑ ƒêang x·ª≠ l√Ω c√¢u h·ªèi...")
        for item in tqdm(data_to_process):
            item_id = item.get("id", item.get("qid")) 

            # B1: G·ªçi h√†m x·ª≠ l√Ω (LLM tr·∫£ v·ªÅ A, B, C, D)
            pred_char, retrieved_context = solve_question(item)
            
            # B2: L∆∞u k·∫øt qu·∫£
            submission_results.append({
                "qid": item_id,
                "answer": pred_char
            })
            
            # B3: T√≠nh ƒëi·ªÉm (So s√°nh tr·ª±c ti·∫øp, kh√¥ng map g√¨ c·∫£)
            if IS_VAL_MODE:
                # ƒê√°p √°n th·∫≠t trong file JSON ƒë√£ l√† ch·ªØ c√°i (A, B, C...)
                true_char = str(item.get('answer', '?')).strip().upper()
                
                if pred_char == true_char:
                    correct_count += 1
                else:
                    wrong_cases.append({
                        "qid": item_id,
                        "question": item['question'],
                        "true_char": true_char,
                        "pred_char": pred_char,
                        "retrieved_context": retrieved_context 
                    })

        # --- 4. GHI FILE ---
        df = pd.DataFrame(submission_results)

        if MODE == "DOCKER" or (MODE == "LOCAL" and IS_FULL_RUN):
            df.to_csv(OUTPUT_FILE_PATH, index=False, encoding='utf-8')
            print(f"\n‚úÖ ƒê√£ l∆∞u file k·∫øt qu·∫£: {OUTPUT_FILE_PATH}")
        else:
            print("\nüíæ Debug xong (Kh√¥ng ghi file CSV).")

        # --- 5. T·ªîNG K·∫æT ---
        print("\n" + "="*40)
        if IS_VAL_MODE:
            acc = (correct_count / len(data_to_process)) * 100
            print(f"üèÜ Accuracy (T·∫≠p Val): {acc:.2f}%")
            
            if wrong_cases:
                pd.DataFrame(wrong_cases).to_csv("wrong_answers.csv", index=False, encoding='utf-8')
                print(f"‚ö†Ô∏è ƒê√£ l∆∞u {len(wrong_cases)} c√¢u sai v√†o 'wrong_answers.csv'")
        elif MODE == "DOCKER":
            print("‚úÖ Docker Run Complete.")
        else:
            print("üèÅ Test Run Complete.")
        print("="*40)
            
    except Exception as e:
        print(f"‚ùå L·ªói Nghi√™m Tr·ªçng: {e}")