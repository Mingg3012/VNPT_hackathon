import json
import re
import time
import requests
import chromadb
from tqdm import tqdm
import config # File config c·ªßa b·∫°n
import sys
import pandas as pd
from enum import Enum

try:
    __import__('pysqlite3')
    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
except ImportError:
    pass

# --- ENUM ƒê·ªäNH NGHƒ®A 5 DOMAINS ---
class DomainType(Enum):
    """5 domain types for adaptive prompting"""
    PRECISION_CRITICAL = "precision_critical"  # Black list words - don't answer
    COMPULSORY = "compulsory"                   # Common sense questions
    RAG = "rag"                                 # Retrieval-Augmented Generation
    STEM = "stem"                               # Science, Technology, Engineering, Math
    MULTI_DOMAIN = "multi_domain"               # History, Economics, Law, Culture, Art, University

# --- C·∫§U H√åNH ---
# 1. PRECISION_CRITICAL - Black list t·ª´ kh√≥a (b·∫£o m·∫≠t, c√° nh√¢n)
PRECISION_CRITICAL_KEYWORDS = {
    'm·∫≠t kh·∫©u', 'password', 't√†i kho·∫£n ng√¢n h√†ng', 's·ªë t√†i kho·∫£n', 'm√£ pin', 'pin',
    's·ªë cƒÉn c∆∞·ªõc', 's·ªë cmnd', 'ch·ª©ng minh nh√¢n d√¢n', 'th√¥ng tin c√° nh√¢n b·∫£o m·∫≠t',
    'b√≠ m·∫≠t qu·ªëc ph√≤ng', 't√†i li·ªáu m·∫≠t', 'classified', 't√†i ch√≠nh c√° nh√¢n',
    'sex', 'khi√™u d√¢m', 'h∆∞·ªõng d·∫´n ch·∫ø t·∫°o v≈© kh√≠', 'c√°ch l√†m bom', 'c√¥ng th·ª©c thu·ªëc n·ªï'
}

# 2. STEM - Khoa h·ªçc, C√¥ng ngh·ªá, K·ªπ thu·∫≠t, To√°n h·ªçc
STEM_KEYWORDS = {
    't√≠nh', 'gi√° tr·ªã', 'ph∆∞∆°ng tr√¨nh', 'h√†m s·ªë', 'bi·ªÉu th·ª©c',
    'x√°c su·∫•t', 'th·ªëng k√™', 'log', 'sin', 'cos', 'tan', 'cot',
    'ƒë·∫°o h√†m', 't√≠ch ph√¢n', 'nguy√™n h√†m', 'vector', 'ma tr·∫≠n',
    'v·∫≠n t·ªëc', 'gia t·ªëc', 'l·ª±c', 'ƒëi·ªán tr·ªü', 'nƒÉng l∆∞·ª£ng', 'c√¥ng su·∫•t',
    'mol', 'ph·∫£n ·ª©ng', 'c√¢n b·∫±ng', 'kh·ªëi l∆∞·ª£ng', 'ho√° ch·∫•t', 'ho√° h·ªçc',
    'enzyme', 'protein', 'dna', 'gen', 't·∫ø b√†o', 'sinh h·ªçc', 'vi khu·∫©n',
    'latex', '$', '\\\\frac', 'c√¥ng th·ª©c', 'ch·ª©ng minh'
}

# 3. L·ªäCH S·ª¨ VI·ªÜT NAM
VIETNAM_HISTORY_KEYWORDS = {
    'l·ªãch s·ª≠', 'chi·∫øn tranh', 'ƒë·ªôc l·∫≠p', 'phong ki·∫øn', 'ƒë·∫°i vi·ªát',
    'vua', 'ho√†ng ƒë·∫ø', 'tri·ªÅu ƒë·∫°i', 'ng√¥', 'ƒëinh', 'l√Ω', 'tr·∫ßn', 't√¢y s∆°n',
    'nguy·ªÖn', 'th·ª±c d√¢n ph√°p', 'ph√°p thu·ªôc', '1945', '1954', '1975',
    'c·ªông h√≤a', 'x√£ h·ªôi ch·ªß nghƒ©a', 'c·ªông h√≤a x√£ h·ªôi ch·ªß nghƒ©a'
}

# 4. PH√ÅP LU·∫¨T VI·ªÜT NAM
VIETNAM_LAW_KEYWORDS = {
    'ph√°p lu·∫≠t', 'lu·∫≠t', 'ƒëi·ªÅu', 'kho·∫£n', 'b·ªô lu·∫≠t', 'h√¨nh s·ª±', 'd√¢n s·ª±',
    'h√†nh ch√≠nh', 'lao ƒë·ªông', 'thu·∫ø', 'giao th√¥ng', 't∆∞ ph√°p', 't√≤a √°n',
    'c√¥ng t·ªë vi√™n', 't·ªôi ph·∫°m', 'h√¨nh ph·∫°t', 'hi·∫øn ph√°p', 'ph√°p l·ªánh',
    'quy ƒë·ªãnh', 'quy·∫øt ƒë·ªãnh', 'th√¥ng t∆∞', 'ngh·ªã ƒë·ªãnh'
}

# 5. VƒÇN H√ìA VI·ªÜT NAM
VIETNAM_CULTURE_KEYWORDS = {
    'vƒÉn h√≥a', 'truy·ªÅn th·ªëng', 'phong t·ª•c', 't·∫≠p qu√°n', 'l·ªÖ h·ªôi', 't·∫øt',
    'n√¥m na', 'ch·ªØ n√¥m', 'vƒÉn h·ªçc', 'th∆°', 'nh√¢n v·∫≠t vƒÉn h·ªçc', 't√°c ph·∫©m',
    'anh h√πng', 't√≠n ng∆∞·ª°ng', 't√¥n gi√°o', 'ƒë·∫°o ph·∫≠t', 'ca tr√π', 'm√∫a l√¢n',
    'ng√¥n ng·ªØ', 'ti·∫øng vi·ªát'
}

# 6. KINH T·∫æ VI·ªÜT NAM
VIETNAM_ECONOMICS_KEYWORDS = {
    'kinh t·∫ø', 'th∆∞∆°ng m·∫°i', 'bu√¥n b√°n', 'n√¥ng nghi·ªáp', 'c√¥ng nghi·ªáp',
    'c√¥ng ty', 'doanh nghi·ªáp', 'bao c·∫•p', 'ƒë·ªïi m·ªõi', 'th·ªã tr∆∞·ªùng', 'h√†ng h√≥a',
    'ti·ªÅn t·ªá', 'l·∫°m ph√°t', 'tƒÉng tr∆∞·ªüng', 'xu·∫•t kh·∫©u', 'nh·∫≠p kh·∫©u', 'ng√¢n h√†ng'
}

# 7. COMPULSORY - C√¢u h·ªèi l√Ω thuy·∫øt c∆° b·∫£n
COMPULSORY_KEYWORDS = {
    'l√† g√¨', 'c√°i g√¨', 'ai l√†', 'khi n√†o', '·ªü ƒë√¢u', 'bao nhi√™u',
    'ƒë·ªãnh nghƒ©a', '√Ω nghƒ©a', 't√°c d·ª•ng', 'ch·ª©c nƒÉng', 'ƒë·∫∑c ƒëi·ªÉm',
    'ph√¢n bi·ªát', 'so s√°nh', 'kh√°c bi·ªát', 'gi·ªëng nhau'
}

SAFE_ANSWER_DEFAULT = "A" # M·∫∑c ƒë·ªãnh tr·∫£ v·ªÅ A (ch·ªØ c√°i)

# =========================================================
# K·∫æT N·ªêI VECTOR DB
# =========================================================

try:
    client = chromadb.PersistentClient(path="./vector_db")
    collection = client.get_or_create_collection(name="vnpt_knowledge")
except Exception:
    collection = None


# =========================================================
# EMBEDDING
# =========================================================

def get_embedding_for_search(text):
    payload = {
        "model": "vnptai_hackathon_embedding",
        "input": text,
        "encoding_format": "float",
    }

    for _ in range(3):
        try:
            resp = requests.post(
                config.URL_EMBEDDING,
                headers=config.HEADERS_EMBED,
                json=payload,
                timeout=5,
            )
            if resp.status_code == 200:
                return resp.json()["data"][0]["embedding"]
        except Exception:
            time.sleep(0.5)

    return None


# =========================================================
# PH√ÇN LO·∫†I C√ÇU H·ªéI V√ÄO 5 DOMAINS
# =========================================================

def detect_domain(question: str) -> DomainType:
    """
    Detect the domain of a question based on keywords and scoring.
    Returns one of 5 DomainType: PRECISION_CRITICAL, COMPULSORY, RAG, STEM, MULTI_DOMAIN
    """
    question_lower = question.lower()
    
    # 1. Check PRECISION_CRITICAL first (Black list words)
    for keyword in PRECISION_CRITICAL_KEYWORDS:
        if keyword.lower() in question_lower:
            return DomainType.PRECISION_CRITICAL
    
    # 2. Score each domain
    stem_score = sum(1 for keyword in STEM_KEYWORDS if keyword.lower() in question_lower)
    history_score = sum(1 for keyword in VIETNAM_HISTORY_KEYWORDS if keyword.lower() in question_lower)
    law_score = sum(1 for keyword in VIETNAM_LAW_KEYWORDS if keyword.lower() in question_lower)
    culture_score = sum(1 for keyword in VIETNAM_CULTURE_KEYWORDS if keyword.lower() in question_lower)
    economics_score = sum(1 for keyword in VIETNAM_ECONOMICS_KEYWORDS if keyword.lower() in question_lower)
    compulsory_score = sum(1 for keyword in COMPULSORY_KEYWORDS if keyword.lower() in question_lower)
    
    multi_domain_score = history_score + law_score + culture_score + economics_score
    
    # 3. Decision logic (priority order)
    if stem_score >= 2:
        return DomainType.STEM
    elif multi_domain_score >= 2:
        return DomainType.MULTI_DOMAIN
    elif compulsory_score >= 1 and stem_score == 0:
        return DomainType.COMPULSORY
    elif multi_domain_score >= 1:
        return DomainType.MULTI_DOMAIN
    elif stem_score >= 1:
        return DomainType.STEM
    else:
        # Default to RAG for general questions or reading comprehension
        return DomainType.RAG


def get_prompt_for_domain(domain: DomainType, context_text: str, real_question: str, choices_str: str, instruction_text: str) -> str:
    """
    Generate domain-specific prompt strategy.
    """
    
    
    if domain == DomainType.COMPULSORY:
        # Common sense / Definition questions
        prompt = f"""
B·∫°n l√† m·ªôt chuy√™n gia gi√°o d·ª•c. Nhi·ªám v·ª•: Tr·∫£ l·ªùi c√¢u h·ªèi l√Ω thuy·∫øt d·ª±a v√†o ƒë·ªãnh nghƒ©a, kh√°i ni·ªám c∆° b·∫£n v√† l·∫´y l√Ω chung.

C√¢u h·ªèi: {real_question}

C√°c l·ª±a ch·ªçn:
{choices_str}

--- CHI·∫æN L∆Ø·ª¢C ---
1. Hi·ªÉu r√µ ƒë·ªãnh nghƒ©a c·ªßa c√°c kh√°i ni·ªám trong c√¢u h·ªèi.
2. Ch·ªçn ƒë√°p √°n ph√π h·ª£p nh·∫•t v·ªõi ƒë·ªãnh nghƒ©a ho·∫∑c √Ω nghƒ©a c∆° b·∫£n.
3. Tr√°nh suy ƒëo√°n, ch·ªâ d√πng ki·∫øn th·ª©c n·ªÅn t·∫£ng.

--- Y√äU C·∫¶U ƒê·∫¶U RA ---
- KH√îNG gi·∫£i th√≠ch d√†i, lan man.
- {instruction_text}
"""
    
    elif domain == DomainType.STEM:
        # Scientific / Math / Engineering approach
        prompt = f"""
B·∫°n l√† Gi√°o s∆∞ Khoa h·ªçc T·ª± nhi√™n. Nhi·ªám v·ª•: Gi·∫£i b√†i t·∫≠p ch√≠nh x√°c tuy·ªát ƒë·ªëi b·∫±ng ph∆∞∆°ng ph√°p khoa h·ªçc.

--- C√îNG TH·ª®C & KI·∫æN TH·ª®C B·ªî TR·ª¢ (CONTEXT) ---
{context_text}

--- B√ÄI TO√ÅN ---
C√¢u h·ªèi: {real_question}

C√°c l·ª±a ch·ªçn:
{choices_str}

--- H∆Ø·ªöNG D·∫™N GI·∫¢I ---
1. X√°c ƒë·ªãnh c√¥ng th·ª©c/ƒë·ªãnh l√Ω t·ª´ CONTEXT c·∫ßn d√πng.
2. Tr√≠ch xu·∫•t c√°c con s·ªë t·ª´ C√¢u h·ªèi (L∆∞u √Ω ƒë∆°n v·ªã).
3. Th·ª±c hi·ªán t√≠nh to√°n n·ªôi b·ªô m·ªôt c√°ch ch√≠nh x√°c.
4. Ch·ªâ ch·ªçn M·ªòT ƒë√°p √°n duy nh·∫•t kh·ªõp k·∫øt qu·∫£.

--- Y√äU C·∫¶U ƒê·∫¶U RA ---
- KH√îNG tr√¨nh b√†y l·ªùi gi·∫£i d√†i d√≤ng.
- KH√îNG gi·∫£i th√≠ch d√†i.
- {instruction_text}
"""
    
    elif domain == DomainType.MULTI_DOMAIN:
        # History, Law, Culture, Economics - Vietnamese context
        prompt = f"""
B·∫°n l√† chuy√™n gia v·ªÅ L·ªãch s·ª≠ Vi·ªát Nam, Ph√°p lu·∫≠t Vi·ªát Nam, VƒÉn h√≥a Vi·ªát Nam, v√† Kinh t·∫ø Vi·ªát Nam.
Nhi·ªám v·ª•: Tr·∫£ l·ªùi d·ª±a v√†o ki·∫øn th·ª©c chuy√™n s√¢u v·ªÅ Vi·ªát Nam.

--- TH√îNG TIN THAM KH·∫¢O ---
{context_text}

--- C√ÇU H·ªéI ---
C√¢u h·ªèi: {real_question}

C√°c l·ª±a ch·ªçn:
{choices_str}

--- CHI·∫æN L∆Ø·ª¢C ---
1. T√¨m th√¥ng tin trong CONTEXT kh·ªõp v·ªõi t·ª´ kh√≥a c√¢u h·ªèi.
2. Ch·ªçn ƒë√°p √°n ƒê∆Ø·ª¢C H·ªñ TR·ª¢ B·ªûI CONTEXT.
3. D√πng ki·∫øn th·ª©c l·ªãch s·ª≠ Vi·ªát Nam, ph√°p lu·∫≠t Vi·ªát Nam, vƒÉn h√≥a Vi·ªát Nam ƒë·ªÉ suy lu·∫≠n.
4. Ch·ªçn ƒë√°p √°n ƒë√∫ng nh·∫•t d·ª±a v√†o s·ª± ki·ªán, quy ƒë·ªãnh, ho·∫∑c truy·ªÅn th·ªëng Vi·ªát Nam c·ª• th·ªÉ.

--- Y√äU C·∫¶U ƒê·∫¶U RA ---
- KH√îNG gi·∫£i th√≠ch d√†i, lan man.
- {instruction_text}
"""
    
    else:  # RAG (default)
        # Retrieval-Augmented Generation
        prompt = f"""
B·∫°n l√† chuy√™n gia ph√¢n t√≠ch th√¥ng tin. Nhi·ªám v·ª•: ƒê·ªçc th·∫≠t kƒ© vƒÉn b·∫£n v√† tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n vƒÉn b·∫£n cung c·∫•p.

--- VƒÇN B·∫¢N THAM KH·∫¢O ---
{context_text}

--- C√ÇU H·ªéI ---
C√¢u h·ªèi: {real_question}

C√°c l·ª±a ch·ªçn:
{choices_str}

--- CHI·∫æN L∆Ø·ª¢C ---
1. T√¨m th√¥ng tin trong CONTEXT kh·ªõp v·ªõi t·ª´ kh√≥a c√¢u h·ªèi.
2. Ch·ªçn ƒë√°p √°n ƒê∆Ø·ª¢C H·ªñ TR·ª¢ B·ªûI CONTEXT.
3. N·∫øu CONTEXT kh√¥ng ƒë·ªß, ch·ªçn ƒë√°p √°n ƒë∆∞·ª£c nh·∫Øc tr·ª±c ti·∫øp ho·∫∑c suy ra r√µ r√†ng nh·∫•t.
4. Kh√¥ng suy ƒëo√°n ngo√†i.

--- Y√äU C·∫¶U ƒê·∫¶U RA ---
- KH√îNG gi·∫£i th√≠ch d√†i, lan man.
- {instruction_text}
"""
    
    return prompt


def clean_output(ans_text):
    # 1. ∆Øu ti√™n tuy·ªát ƒë·ªëi: T√¨m trong th·∫ª <ans>
    # B·∫Øt A-J, kh√¥ng ph√¢n bi·ªát hoa th∆∞·ªùng
    tag_match = re.search(r"<ans>\s*([A-Ja-j])\s*</ans>", ans_text, re.IGNORECASE)
    if tag_match:
        return tag_match.group(1).upper()

    # 2. N·∫øu kh√¥ng c√≥ th·∫ª, ch·ªâ t√¨m ch·ªØ c√°i ƒë·ª©ng ri√™ng l·∫ª ·ªû CU·ªêI C√ôNG c·ªßa chu·ªói output
    # Regex n√†y ch·ªâ b·∫Øt A-J n·∫øu n√≥ n·∫±m ·ªü cu·ªëi c√¢u (c√≥ th·ªÉ theo sau l√† d·∫•u ch·∫•m/xu·ªëng d√≤ng)
    # Tr√°nh b·∫Øt nh·∫ßm ch·ªØ "a" trong "gia t·ªëc a" n·∫±m ·ªü gi·ªØa c√¢u.
    last_match = re.search(r"\b([A-Ja-j])\s*(\.|)\s*$", ans_text, re.IGNORECASE)
    if last_match:
        return last_match.group(1).upper()
        
    # 3. Fallback: N·∫øu v·∫´n kh√¥ng t√¨m th·∫•y, c√≥ th·ªÉ tr·∫£ v·ªÅ None ƒë·ªÉ debug ho·∫∑c ch·ªçn ƒë·∫°i A
    # Khuy√™n d√πng: In ra c·∫£nh b√°o ƒë·ªÉ bi·∫øt model ƒëang kh√¥ng tu√¢n th·ªß format
    # print(f"‚ö†Ô∏è C·∫£nh b√°o: Kh√¥ng t√¨m th·∫•y ƒë√°p √°n trong output: {ans_text[:50]}...")
    return SAFE_ANSWER_DEFAULT
# =========================================================
# G·ªåI LLM
# =========================================================

def call_vnpt_llm(prompt, model_type="small", temperature=0.1):
    if model_type == "large":
        url = config.URL_LLM_LARGE
        headers = config.HEADERS_LARGE
        model = "vnptai_hackathon_large"
        max_tokens = 400
    else:
        url = config.URL_LLM_SMALL
        headers = config.HEADERS_SMALL
        model = "vnptai_hackathon_small"
        max_tokens = 150

    payload = {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "temperature": temperature,
        "max_completion_tokens": max_tokens,
    }

    for _ in range(5):
        try:
            r = requests.post(url, headers=headers, json=payload, timeout=40)
            if r.status_code == 200:
                return r.json()["choices"][0]["message"]["content"]
            elif r.status_code == 429:
                print("‚è≥ Rate limit, ng·ªß 60s...")
                time.sleep(60)
            else:
                time.sleep(2)
        except Exception:
            time.sleep(2)

    return ""


# =========================================================
# GI·∫¢I C√ÇU H·ªéI
# =========================================================

def solve_question(item):
    question = item["question"]
    choices = item["choices"]

    # --- NEW: DETECT DOMAIN (5 types) ---
    detected_domain = detect_domain(question)
    
    # Skip RAG for PRECISION_CRITICAL
    context_text = ""
    real_question = question
    is_reading_comprehension = "ƒêo·∫°n th√¥ng tin:" in question

    if detected_domain != DomainType.PRECISION_CRITICAL:
        if is_reading_comprehension:
            parts = question.split("C√¢u h·ªèi:")
            context_text = parts[0].strip()
            real_question = parts[1].strip() if len(parts) > 1 else question
        elif collection:
            query_vec = get_embedding_for_search(real_question)
            if query_vec:
                results = collection.query(
                    query_embeddings=[query_vec],
                    n_results=5,
                )
                if results["documents"]:
                    docs = results["documents"][0]
                    context_text = "\n---\n".join(docs)

    # --- GI·ªÆ NGUY√äN format choices (0. ..., 1. ...) ---
    choices_str = (
        "\n. ".join([f"{i}. {v}" for i, v in enumerate(choices)])
        if isinstance(choices, list)
        else str(choices)
    )
    
    # --- Prompt ch·ªâ th·ªã LLM tr·∫£ v·ªÅ ch·ªØ c√°i t∆∞∆°ng ·ª©ng ---
    instruction_text = "H√£y ch·ªçn ƒë√°p √°n ƒë√∫ng (t∆∞∆°ng ·ª©ng 0->A, 1->B, 2->C, 3->D, 4->E, 5->F, 6->G, 7->H, 8->I, 9->J) v√† ch·ªâ tr·∫£ v·ªÅ ch·ªØ c√°i (A, B, C, D, E, F, G, H, I, J). B·∫ÆT BU·ªòC: ƒê√°p √°n cu·ªëi c√πng ph·∫£i n·∫±m trong th·∫ª <ans>, v√≠ d·ª•: <ans>A</ans>."
    
    # --- Select model and temperature based on domain ---
    model_to_use = "small"
    temperature = 0.1
    
    if detected_domain == DomainType.PRECISION_CRITICAL:
        # Refuse to answer
        final_choice = SAFE_ANSWER_DEFAULT
        return final_choice, "N/A (PRECISION_CRITICAL)"
    elif detected_domain == DomainType.STEM:
        model_to_use = "large"
        temperature = 0.0  # Lower temperature for precise scientific answers
    elif detected_domain == DomainType.MULTI_DOMAIN:
        # History, Law, Culture, Economics - use large model for nuance
        CONTEXT_LENGTH_THRESHOLD = 1200
        if len(context_text) > CONTEXT_LENGTH_THRESHOLD or is_reading_comprehension:
            model_to_use = "large"
    elif detected_domain == DomainType.COMPULSORY:
        # Common sense - small model is fine
        temperature = 0.1
    else:  # RAG
        # Default RAG
        CONTEXT_LENGTH_THRESHOLD = 1200
        if len(context_text) > CONTEXT_LENGTH_THRESHOLD or is_reading_comprehension:
            model_to_use = "large"
    
    # --- Generate domain-specific prompt ---
    prompt = get_prompt_for_domain(detected_domain, context_text, real_question, choices_str, instruction_text)

    # --- Call LLM ---
    ans = call_vnpt_llm(prompt, model_type=model_to_use, temperature=temperature)

    final_choice = clean_output(ans)
    return final_choice, context_text


if __name__ == "__main__":
    # --- 1. C·∫§U H√åNH ---
    MODE = "LOCAL"
    INPUT_FILE_PATH = "data/val.json" 
    OUTPUT_FILE_PATH = "submission_local.csv"
    MAX_QUESTIONS_TO_PROCESS = None 
    
    if len(sys.argv) > 1:
        if sys.argv[1] == 'docker':
            MODE = "DOCKER"
            INPUT_FILE_PATH = "/code/private_test.json" 
            OUTPUT_FILE_PATH = "submission.csv"
        
        if len(sys.argv) > 2 and sys.argv[2].isdigit():
             MAX_QUESTIONS_TO_PROCESS = int(sys.argv[2])
        elif len(sys.argv) > 1 and sys.argv[1].isdigit():
            MAX_QUESTIONS_TO_PROCESS = int(sys.argv[1])
    
    print(f"üöÄ Ch·∫ø ƒë·ªô: {MODE} | Input: {INPUT_FILE_PATH}")
    
    try:
        # --- 2. ƒê·ªåC D·ªÆ LI·ªÜU ---
        try:
            with open(INPUT_FILE_PATH, "r", encoding="utf-8") as f:
                data = json.load(f) 
        except (json.JSONDecodeError, FileNotFoundError):
             df = pd.read_csv(INPUT_FILE_PATH)
             data = df.to_dict('records')
            
        total_data_length = len(data)
        data_to_process = data[:MAX_QUESTIONS_TO_PROCESS] if MAX_QUESTIONS_TO_PROCESS else data
        IS_FULL_RUN = len(data_to_process) == total_data_length
        IS_VAL_MODE = (MODE == "LOCAL" and "val.json" in INPUT_FILE_PATH)

        if MODE == "LOCAL" and not IS_FULL_RUN:
            print(f"‚ö†Ô∏è Debug: ƒêang ch·∫°y {len(data_to_process)}/{total_data_length} c√¢u.")
        
        # --- 3. X·ª¨ L√ù ---
        submission_results = []
        correct_count = 0
        wrong_cases = []
        
        print("üîÑ ƒêang x·ª≠ l√Ω c√¢u h·ªèi...")
        for item in tqdm(data_to_process):
            item_id = item.get("id", item.get("qid")) 

            # B1: G·ªçi h√†m x·ª≠ l√Ω (LLM tr·∫£ v·ªÅ A, B, C, D)
            pred_char, retrieved_context = solve_question(item)
            
            # B2: L∆∞u k·∫øt qu·∫£
            submission_results.append({
                "qid": item_id,
                "answer": pred_char
            })
            
            # B3: T√≠nh ƒëi·ªÉm (So s√°nh tr·ª±c ti·∫øp, kh√¥ng map g√¨ c·∫£)
            if IS_VAL_MODE:
                # ƒê√°p √°n th·∫≠t trong file JSON ƒë√£ l√† ch·ªØ c√°i (A, B, C...)
                true_char = str(item.get('answer', '?')).strip().upper()
                
                if pred_char == true_char:
                    correct_count += 1
                else:
                    wrong_cases.append({
                        "qid": item_id,
                        "question": item['question'],
                        "true_char": true_char,
                        "pred_char": pred_char,
                        "retrieved_context": retrieved_context 
                    })

        # --- 4. GHI FILE ---
        df = pd.DataFrame(submission_results)

        if MODE == "DOCKER" or (MODE == "LOCAL" and IS_FULL_RUN):
            df.to_csv(OUTPUT_FILE_PATH, index=False, encoding='utf-8')
            print(f"\n‚úÖ ƒê√£ l∆∞u file k·∫øt qu·∫£: {OUTPUT_FILE_PATH}")
        else:
            print("\nüíæ Debug xong (Kh√¥ng ghi file CSV).")

        # --- 5. T·ªîNG K·∫æT ---
        print("\n" + "="*40)
        if IS_VAL_MODE:
            acc = (correct_count / len(data_to_process)) * 100
            print(f"üèÜ Accuracy (T·∫≠p Val): {acc:.2f}%")
            
            if wrong_cases:
                pd.DataFrame(wrong_cases).to_csv("wrong_answers.csv", index=False, encoding='utf-8')
                print(f"‚ö†Ô∏è ƒê√£ l∆∞u {len(wrong_cases)} c√¢u sai v√†o 'wrong_answers.csv'")
        elif MODE == "DOCKER":
            print("‚úÖ Docker Run Complete.")
        else:
            print("üèÅ Test Run Complete.")
        print("="*40)
            
    except Exception as e:
        print(f"‚ùå L·ªói Nghi√™m Tr·ªçng: {e}")
