import requests
from bs4 import BeautifulSoup
import re
import os
import time

# --- Cáº¤U HÃŒNH ---
FILE_PATH = "data/documents.txt"

wiki_urls = [
    "https://vi.wikipedia.org/wiki/HÃ³a_há»c",
    "https://vi.wikipedia.org/wiki/Báº£ng_tuáº§n_hoÃ n",
    "https://vi.wikipedia.org/wiki/Pháº£n_á»©ng_hÃ³a_há»c",
    "https://vi.wikipedia.org/wiki/HÃ³a_há»c_há»¯u_cÆ¡",
    "https://vi.wikipedia.org/wiki/Sinh_há»c",
    "https://vi.wikipedia.org/wiki/Di_truyá»n_há»c",
    "https://vi.wikipedia.org/wiki/Táº¿_bÃ o",
    "https://vi.wikipedia.org/wiki/Tiáº¿n_hÃ³a",
    "https://vi.wikipedia.org/wiki/Lá»‹ch_sá»­_tháº¿_giá»›i",
    "https://vi.wikipedia.org/wiki/Chiáº¿n_tranh_tháº¿_giá»›i_thá»©_hai",
    "https://vi.wikipedia.org/wiki/Äá»‹a_lÃ½",
    "https://vi.wikipedia.org/wiki/CÃ´ng_nghá»‡_thÃ´ng_tin",
    "https://vi.wikipedia.org/wiki/TrÃ­_tuá»‡_nhÃ¢n_táº¡o",
    "https://vi.wikipedia.org/wiki/VÄƒn_há»c"
]

def get_start_id(file_path):
    if not os.path.exists(file_path):
        return 1
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read()
            matches = re.findall(r"\.", content)
            if matches:
                return int(max(map(int, matches))) + 1
    except Exception:
        pass
    return 1

def clean_wiki_text(text):
    """LÃ m sáº¡ch vÄƒn báº£n Wikipedia"""
    # XÃ³a cÃ¡c tham chiáº¿u vÃ  kÃ½ tá»± thá»«a
    text = re.sub(r'\[\d+\]', '', text) 
    text = re.sub(r'\[cáº§n dáº«n nguá»“n\]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def crawl_wiki_topics():
    current_id = get_start_id(FILE_PATH)
    print(f"ğŸš€ Báº¯t Ä‘áº§u crawl HÃ³a/Sinh. ID tiáº¿p theo: ")
    
    new_content = ""
    headers = {'User-Agent': 'Mozilla/5.0'}

    for url in wiki_urls:
        print(f"--> Äang táº£i: {url}")
        try:
            resp = requests.get(url, headers=headers, timeout=10)
            if resp.status_code != 200:
                print(f"âŒ Lá»—i táº£i trang: {url}")
                continue

            soup = BeautifulSoup(resp.content, 'html.parser')
            
            # 1. Láº¥y tiÃªu Ä‘á»
            title = soup.find('h1', {'id': 'firstHeading'}).text
            new_content += f"CHá»¦ Äá»€: {title}\n"
            current_id += 1
            new_content += f"Link: {url}\n"
            current_id += 1

            # 2. Láº¥y ná»™i dung chÃ­nh (div class="mw-parser-output")
            content_div = soup.find('div', {'class': 'mw-parser-output'})
            if content_div:
                
                # --- PHÆ¯Æ NG PHÃP Má»šI: Láº¥y táº¥t cáº£ text vÃ  split theo dÃ²ng ---
                
                # Loáº¡i bá» cÃ¡c box bÃªn pháº£i (infobox) vÃ  cÃ¡c má»¥c lá»¥c (toc) Ä‘á»ƒ giáº£m rÃ¡c
                for junk in content_div.find_all(['table', 'div', 'ul'], class_=['infobox', 'toc', 'navbox']):
                    junk.decompose()
                
                # Láº¥y text thÃ´, dÃ¹ng '\n' lÃ m dáº¥u phÃ¢n cÃ¡ch giá»¯a cÃ¡c block
                full_text = content_div.get_text(separator="\n\n")

                # TÃ¡ch thÃ nh cÃ¡c Ä‘oáº¡n vÄƒn
                paragraphs = full_text.split('\n\n')

                count = 0
                for p_raw in paragraphs:
                    text = clean_wiki_text(p_raw)
                    
                    # Lá»c máº¡nh máº½ hÆ¡n: Chá»‰ láº¥y Ä‘oáº¡n vÄƒn cÃ³ ná»™i dung > 150 kÃ½ tá»±
                    # vÃ  khÃ´ng pháº£i lÃ  cÃ¡c má»¥c (vÃ¬ Ä‘Ã£ loáº¡i h2, ul thÃ´ á»Ÿ trÃªn)
                    if len(text) > 150 and not text.endswith('Má»¥c lá»¥c'):
                        new_content += f"{text}\n"
                        current_id += 1
                        count += 1
                    
                    # Giá»›i háº¡n láº¥y 15 Ä‘oáº¡n vÄƒn cháº¥t lÆ°á»£ng cao má»—i bÃ i
                    if count >= 15:
                        break
                
                print(f"âœ… ÄÃ£ thÃªm {count} Ä‘oáº¡n vÄƒn vá» {title}.")
            
            time.sleep(1) 

        except Exception as e:
            print(f"âŒ Lá»—i ngoáº¡i lá»‡: {e}")

    # Ghi vÃ o file
    if new_content:
        with open(FILE_PATH, "a", encoding="utf-8") as f:
            f.write("\n" + new_content)
        print(f"\nğŸ‰ ThÃ nh cÃ´ng! ÄÃ£ cáº­p nháº­t kiáº¿n thá»©c má»›i vÃ o '{FILE_PATH}'")
    else:
        print("\nâš ï¸ KhÃ´ng táº£i Ä‘Æ°á»£c ná»™i dung nÃ o.")

if __name__ == "__main__":
    crawl_wiki_topics()