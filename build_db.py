import os
import re
import time
import requests
import chromadb
from tqdm import tqdm
import config
import sys
import pandas as pd

# --- FIX L·ªñI CHROMA DB TR√äN DOCKER ---
try:
    __import__('pysqlite3')
    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
except ImportError:
    pass

# --- C·∫§U H√åNH ---
DB_PATH = "./vector_db"
DATA_SOURCE = "data/documents.txt"
MAX_CHUNK_SIZE = 1200 # K√Ω t·ª± t·ªëi ƒëa cho 1 chunk (An to√†n cho Embedding)

def clean_text(text):
    # X√≥a kho·∫£ng tr·∫Øng th·ª´a, tab, xu·ªëng d√≤ng l·ªôn x·ªôn
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def split_long_text(text, limit=1000):
    """Chia ƒëo·∫°n vƒÉn d√†i th√†nh c√°c ƒëo·∫°n nh·ªè h∆°n nh∆∞ng v·∫´n gi·ªØ nghƒ©a"""
    if len(text) <= limit:
        return [text]
    
    # C·∫Øt theo c√¢u (.) ƒë·ªÉ kh√¥ng b·ªã ƒë·ª©t g√£y √Ω
    sentences = text.split('. ')
    chunks = []
    current_chunk = ""
    
    for sentence in sentences:
        if len(current_chunk) + len(sentence) < limit:
            current_chunk += sentence + ". "
        else:
            chunks.append(current_chunk.strip())
            current_chunk = sentence + ". "
            
    if current_chunk:
        chunks.append(current_chunk.strip())
        
    return chunks

def get_embedding(text):
    payload = {
        "model": "vnptai_hackathon_embedding",
        "input": text,
        "encoding_format": "float"
    }
    # Retry 3 l·∫ßn
    for _ in range(3):
        try:
            response = requests.post(config.URL_EMBEDDING, headers=config.HEADERS_EMBED, json=payload, timeout=15)
            if response.status_code == 200:
                return response.json()['data'][0]['embedding']
            elif response.status_code == 429: # Rate limit
                time.sleep(2)
        except:
            pass
        time.sleep(0.5)
    return None

def build_database():
    print("üöÄ B·∫Øt ƒë·∫ßu x√¢y d·ª±ng Vector Database (B·∫£n T·ªëi ∆∞u)...")
    
    # 1. K·∫øt n·ªëi v√† Reset DB
    client = chromadb.PersistentClient(path=DB_PATH)
    try:
        client.delete_collection("vnpt_knowledge")
        print("üóëÔ∏è ƒê√£ x√≥a Collection c≈©.")
    except:
        pass
        
    collection = client.get_or_create_collection(name="vnpt_knowledge")

    # 2. ƒê·ªçc file
    if not os.path.exists(DATA_SOURCE):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file {DATA_SOURCE}")
        return

    with open(DATA_SOURCE, "r", encoding="utf-8") as f:
        full_text = f.read()
        # B∆∞·ªõc 1: T√°ch theo ƒëo·∫°n vƒÉn l·ªõn (\n\n)
        raw_paragraphs = full_text.split('\n\n') 
        
    # 3. X·ª≠ l√Ω & L√†m s·∫°ch & C·∫Øt nh·ªè
    final_docs = []
    print("‚öôÔ∏è ƒêang x·ª≠ l√Ω v√† l√†m s·∫°ch d·ªØ li·ªáu...")
    
    for p in raw_paragraphs:
        cleaned = clean_text(p)
        if len(cleaned) < 20: continue # B·ªè qua r√°c
        
        # N·∫øu ƒëo·∫°n v·∫´n qu√° d√†i > 1200 k√Ω t·ª± -> C·∫Øt nh·ªè ti·∫øp
        if len(cleaned) > MAX_CHUNK_SIZE:
            sub_chunks = split_long_text(cleaned, limit=MAX_CHUNK_SIZE)
            final_docs.extend(sub_chunks)
        else:
            final_docs.append(cleaned)
    
    print(f"üìÑ S·ªë l∆∞·ª£ng ƒëo·∫°n vƒÉn sau khi x·ª≠ l√Ω: {len(final_docs)}")
    
    # 4. N·∫°p v√†o DB
    batch_size = 10 
    ids_batch = []
    docs_batch = []
    emb_batch = []
    metadatas_batch = []
    
    print("embedding...")
    for i, doc in tqdm(enumerate(final_docs), total=len(final_docs)):
        emb = get_embedding(doc)
        
        if emb:
            ids_batch.append(f"doc_{i}")
            docs_batch.append(doc)
            emb_batch.append(emb)
            metadatas_batch.append({"source": "manual_ingest", "length": len(doc)})
        
        # Batch insert
        if len(ids_batch) >= batch_size:
            collection.add(
                ids=ids_batch, 
                embeddings=emb_batch, 
                documents=docs_batch,
                metadatas=metadatas_batch
            )
            ids_batch, docs_batch, emb_batch, metadatas_batch = [], [], [], []
            time.sleep(0.1) # Gi·ªØ t·ªëc ƒë·ªô an to√†n

    # N·∫°p n·ªët s·ªë l·∫ª
    if ids_batch:
        collection.add(
            ids=ids_batch, 
            embeddings=emb_batch, 
            documents=docs_batch,
            metadatas=metadatas_batch
        )

    print(f"\n‚úÖ HO√ÄN T·∫§T! T·ªïng s·ªë documents trong DB: {collection.count()}")

if __name__ == "__main__":
    build_database()